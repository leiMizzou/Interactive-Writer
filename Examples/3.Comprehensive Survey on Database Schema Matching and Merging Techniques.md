# Comprehensive Survey on Database Schema Matching and Merging Techniques

## 1 Introduction to Database Schema Matching and Merging

### 1.1 Importance of Schema Matching and Merging in Data Integration

Schema matching and merging are critical components of data integration, enabling the consolidation of information from disparate data sources into a unified view. This process is essential for various applications, including data cleaning, bioinformatics, pattern recognition, and decision-making in large enterprises [1][2]. In the era of big data, the challenge of deriving a global schema from massive data sources is exacerbated, necessitating efficient and effective schema integration algorithms [1]. These algorithms must account for the representation differences in attribute names across various data sources, employing techniques such as ED Join and Semantic Join to facilitate accurate integration [1].

The importance of schema matching extends beyond mere data transfer; it also supports human decision-makers and planners who rely on information derived from schema matchers [2]. In large enterprises, schema matching plays a valuable role independent of mapping construction, highlighting the need for robust schema matching technology [2]. For instance, in the context of vegetation data integration, semi-automated hybrid schema matching frameworks have been developed to address the challenges of integrating heterogeneous data sources, leveraging both schema-level and instance-level matching to enhance accuracy and efficiency [3].

Moreover, the integration of schema matching with advanced technologies like deep learning and large language models (LLMs) is emerging as a promising approach to automate and enhance the schema matching process [5][4]. These technologies can reduce manual effort and improve the quality of matching results, particularly in scenarios where data is noisy or schema changes are frequent [5]. Additionally, the use of LLMs for schema matching has shown potential in bootstrapping the process and assisting data engineers in identifying semantic correspondences solely based on schema element names and descriptions [4].

In summary, schema matching and merging are pivotal for data integration, enabling the harmonization of data from diverse sources into a coherent and usable format. The development of sophisticated algorithms, the incorporation of advanced technologies, and the recognition of schema matching's broader role in decision-making underscore its significance in contemporary data management [1][2][3][5][4].

### 1.2 Definitions and Key Concepts

In the context of database schema matching and merging, it is essential to establish a clear understanding of the fundamental concepts and key terms that underpin these processes. **Schema matching** refers to the task of identifying correspondences between elements of different database schemas, such as tables, columns, and relationships. This process is crucial for enabling interoperability between heterogeneous data sources [13]. **Schema merging**, on the other hand, involves combining multiple schemas into a unified schema that preserves the integrity and semantics of the original schemas [9].

Key concepts in schema matching and merging include **semantic similarity**, which measures the degree to which two schema elements are conceptually alike [14]. This can be achieved through various techniques such as string matching, structural analysis, and semantic analysis using ontologies or thesauri [16]. Another critical concept is **schema alignment**, which involves determining the best mapping between schema elements to minimize redundancy and maximize information preservation [17].

**Ontologies** play a significant role in schema matching and merging by providing a formal, explicit specification of concepts and relationships within a domain [16]. They facilitate the semantic interpretation of schema elements, thereby enhancing the accuracy of matching and merging processes [15]. Additionally, **concept lattices** and **formal contexts** are mathematical structures that can be used to represent and analyze the relationships between schema elements, aiding in the identification of correspondences and the construction of a unified schema [9].

The process of schema matching and merging also involves understanding the **logical and topological structures** of schemas, which can be modeled using set theory and topology [7]. These structures help in defining the relationships between schema elements and in ensuring that the merged schema maintains the necessary constraints and integrity [15].

Finally, the concept of **conceptual spaces** provides a framework for representing and combining concepts in a way that is both intuitive and mathematically rigorous, which is particularly useful in the context of schema merging where complex relationships need to be preserved [9]. By leveraging these foundational concepts, the processes of schema matching and merging can be effectively and efficiently executed, leading to more robust and interoperable database systems [9].

### 1.3 Challenges in Schema Matching and Merging

Schema matching and merging present several significant challenges that complicate the process of integrating heterogeneous data sources. One of the primary difficulties is **semantic heterogeneity**, which arises from the use of different terminologies, structures, and data models across various data sources [29][25][4]. This semantic gap necessitates sophisticated techniques to identify and align corresponding elements, often requiring the use of external knowledge bases or domain-specific ontologies [30][20].

Another challenge is the **scale and complexity** of modern data sources, which can involve large-scale databases with numerous attributes and relationships [2][1]. The sheer volume of data and the intricate relationships between elements can make manual matching and merging infeasible, necessitating automated solutions that can handle large-scale data integration tasks efficiently [18][31].

The **dynamic nature of data schemas** also poses a significant challenge, as schemas can evolve over time due to changes in data requirements, business processes, or technological advancements [18][5]. This necessitates mechanisms that can adapt to schema changes, ensuring that data integration processes remain effective and up-to-date [32][22].

**Uncertainty and ambiguity** in schema matching are further complicating factors. The inherent uncertainty in identifying correspondences between schema elements can lead to incorrect matches, which can propagate errors throughout the data integration process [24][31]. Addressing this uncertainty requires robust verification and validation mechanisms, often involving human-in-the-loop approaches to refine and correct automated matches [21][19].

Finally, the integration of schema matching and merging with other data integration tasks, such as data cleaning and transformation, adds another layer of complexity. Ensuring that these tasks are coordinated and that the integrated data is consistent and accurate is a significant challenge that requires a holistic approach to data integration [30][23].

In summary, the challenges in schema matching and merging include semantic heterogeneity, scale and complexity, schema evolution, uncertainty and ambiguity, and the need for coordinated integration with other data integration tasks. Addressing these challenges requires innovative techniques and methodologies that can effectively handle the intricacies of modern data integration scenarios [28][26][33].

### 1.4 Real-World Applications and Use Cases

Database schema matching and merging techniques are indispensable for integrating heterogeneous data sources across various industries and domains. In the healthcare sector, these techniques are crucial for matching and merging patient records from multiple hospitals or clinics, ensuring accurate and comprehensive patient histories [41]. Similarly, in finance, schema matching and merging consolidate transaction data from various banking systems, maintaining the accuracy of financial reports [43].

In e-commerce, these techniques integrate product catalogs from different vendors, providing a unified shopping experience. For example, when a customer searches for a product, the system must match and merge product descriptions, prices, and availability from multiple sources to offer a comprehensive result [40].

Smart cities benefit from these techniques by integrating data from IoT devices and sensors to enhance urban management. Traffic management systems, for instance, aggregate data from various traffic sensors and cameras through schema matching and merging, enabling efficient traffic flow and incident response [37].

Scientific research also relies on these techniques to integrate data from various experiments and studies, facilitating meta-analyses and collaborative research efforts [42]. In genomics, matching and merging genetic data from different studies is essential for identifying patterns and correlations that could lead to new discoveries [43].

In the education sector, schema matching and merging integrate student records from different schools or educational institutions, ensuring accurate tracking and reporting of student data [38]. This is vital for government agencies and educational bodies to maintain comprehensive and up-to-date records of student performance and enrollment [35].

In summary, the real-world applications of database schema matching and merging are extensive, spanning healthcare, finance, e-commerce, smart cities, scientific research, and education. These techniques ensure data integrity, enable data-driven decision-making, and facilitate seamless data integration across diverse systems and domains.

### 1.5 Historical Context and Development of Schema Theory


The historical development of schema theory began in the early 20th century as a mathematical construct aimed at providing a theoretical foundation for genetic algorithms [45]. Early work focused on elementary probability theory, laying the groundwork for more complex applications. The 1990s broadened schema theory's scope, integrating it with brain theory schemas, grid automata, and block-schemas, demonstrating its versatility across neurophysiology, psychology, and computer science [44]. This period also saw the development of the Exact Schema Theorem [49], which provided precise formulas for the expected population fraction in a schema, enhancing theoretical rigor.

In the early 21st century, schema theory intersected with artificial intelligence and cognitive science, notably through the Winograd Schema Challenge [46], which tested machine intelligence by evaluating coreference resolution. Human-machine collaboration frameworks [46] further highlighted schema theory's potential in advancing AI capabilities. Recent years have seen schema theory applied to practical contexts, such as database schema matching and merging [47], and the development of event schemas for knowledge discovery [48].

This historical trajectory underscores schema theory's evolution from a theoretical construct in mathematics to a versatile framework applicable in diverse scientific and technological domains. Its adaptability and robustness continue to provide valuable insights and tools for understanding complex systems and phenomena, emphasizing the importance of interdisciplinary research and continuous theoretical refinement.


### 1.6 Role of Background Knowledge in Schema Matching

Background knowledge plays a pivotal role in the process of database schema matching and merging, enhancing the accuracy and efficiency of these tasks. The integration of external knowledge sources, such as ontologies, thesauri, and domain-specific knowledge graphs, can significantly improve the automatic identification of semantic correspondences between schema elements. This is particularly evident in the context of large-scale enterprises, where schema matching is not merely a precursor to data transfer but a critical tool for human decision-makers.

The use of background knowledge can be categorized into two main strategies: explicit and latent. Explicit strategies, which directly apply predefined rules or mappings, tend to outperform latent strategies that infer correspondences indirectly from data. For instance, knowledge graphs like BabelNet have been shown to consistently yield good results in schema matching tasks, often performing competitively with state-of-the-art matching systems without the need for dataset-specific optimizations.

Moreover, background knowledge can be leveraged to address the challenges posed by heterogeneous and noisy data sources. By incorporating linked data and leveraging rich types from sources like Freebase, schema matching algorithms can significantly enhance the quality of matching results, leading to more informed business decisions.

In the realm of machine learning, background knowledge can guide the training of models, improving their performance on tail queries and documents with insufficient click-through data. By incorporating semantic knowledge into the objective function, models can better generalize and enhance their relevance predictions, particularly for less frequent queries.

Furthermore, the dynamic integration of background knowledge in neural natural language understanding (NLU) systems can enhance their performance on tasks such as document question answering and recognizing textual entailment. By dynamically incorporating explicit background knowledge in the form of free-text statements, these systems can achieve more semantically appropriate results.

In summary, background knowledge is a crucial component in the advancement of schema matching and merging technologies. Its effective utilization can lead to more accurate, efficient, and interpretable systems, thereby enhancing the overall data integration process. This integration is particularly valuable in complex scenarios where human expertise and real-time feedback are crucial for ensuring the accuracy and relevance of integrated data.

### 1.7 Integration of Schema Matching with Visual Data Analysis

The integration of schema matching with visual data analysis represents a significant advancement in the field of data integration and analysis. This approach leverages the strengths of both schema matching algorithms and visual analytics tools to enhance the efficiency and effectiveness of data integration processes. Visual data analysis provides a user-friendly interface that allows analysts to interactively explore and understand complex data structures, while schema matching algorithms automate the identification of correspondences between different data schemas.

One of the key benefits of this integration is the ability to provide real-time feedback and iterative refinement of matching results. For instance, [52] demonstrated that in-situ integration, where data integration is seamlessly merged with visual analytics operations, allows users to spend more time on analysis tasks compared to ex-situ integration. This approach enables users to visually inspect and validate the matching results, thereby reducing the likelihood of errors and improving the overall quality of the integrated data.

Moreover, visual analytics can help in identifying patterns and anomalies that might be missed by purely algorithmic approaches. For example, [53] proposed a method that combines visual analytics with content-based data retrieval technology, allowing analysts to select regions of interest in the data domain and feed the results into multiple visualization workspaces. This approach not only improves data perception but also enhances analytical possibilities by breaking down complex data analysis into smaller, more manageable problems.

The integration of schema matching with visual data analysis also facilitates the incorporation of domain-specific knowledge and user expertise into the matching process. For instance, [54] introduced MELT Dashboard, an interactive Web user interface for ontology alignment evaluation, which allows for self-service analyses and detailed group evaluations. This tool enables users to drill down into the matcher performance and explore the results interactively, thereby leveraging their domain knowledge to refine the matching process.

In conclusion, the integration of schema matching with visual data analysis offers a powerful framework for enhancing data integration and analysis. By combining the strengths of both approaches, this integration enables more accurate, efficient, and user-friendly data integration processes, ultimately leading to better insights and decision-making. This approach is particularly valuable in complex scenarios where human expertise and real-time feedback are crucial for ensuring the accuracy and relevance of integrated data.

### 1.8 Logic-Based Approaches to Data Integration

Logic-based approaches to data integration leverage formal logic systems to address the challenges of integrating data from heterogeneous sources. These methods are particularly effective in scenarios where the data sources have complex, interrelated structures and where semantic consistency is crucial. Logic-based approaches often involve the use of logical rules and constraints to map and reconcile data from different schemas, ensuring that the integrated data maintains its integrity and coherence.

One of the key advantages of logic-based approaches is their ability to handle complex queries and transformations through the use of logical inference mechanisms. For instance, [56] presents a logic-based approach using resolution to transform queries from database relations to external resources, addressing issues such as integrity constraints, negation, and recursion. This method provides a uniform framework that encompasses previous work on query folding and answering queries using views.

Another significant aspect of logic-based data integration is the integration of diverse data sources with no a priori common schema. [57] discusses the role of computational logic in information integration, emphasizing the need for novel techniques that bridge databases and artificial intelligence. This approach allows for the synthesis of information from heterogeneous sources in a flexible and transparent manner, enabling more informed decision-making.

Logic-based methods also play a crucial role in addressing the challenges of semantic heterogeneity. [58] proposes a stratified data integration approach where data is organized into independent representation layers, each dealing with a specific type of heterogeneity (conceptual, language, knowledge, and data). This stratification allows for a uniform treatment of different types of heterogeneity within each layer, facilitating the integration process.

Moreover, logic-based approaches are instrumental in ensuring the consistency and correctness of integrated data. [55] introduces Euler/X, a toolkit for logic-based taxonomy integration that provides tools for detecting, explaining, and reconciling inconsistencies between taxonomies. This approach leverages various reasoning systems, including first-order reasoners and answer set programming, to ensure the coherence of integrated taxonomies.

In summary, logic-based approaches offer a robust and flexible framework for data integration, enabling the handling of complex queries, the synthesis of heterogeneous data sources, and the maintenance of semantic consistency. These methods are essential for ensuring that integrated data is accurate, coherent, and useful for decision-making processes. The integration of logic-based techniques with visual data analysis further enhances the efficiency and effectiveness of data integration, leveraging human expertise and real-time feedback to refine and validate matching results.

## 2 Methods and Techniques for Database Schema Matching and Merging

### 2.1 Rule-Based Methods

Rule-based methods for database schema matching and merging leverage predefined rules to identify and reconcile discrepancies between schemas. These methods are particularly effective in scenarios where domain-specific knowledge is well-defined and can be codified into rules. The rules can be either manually crafted by domain experts or automatically generated through machine learning techniques.

One of the key advantages of rule-based methods is their interpretability. Since the rules are explicit and often human-readable, they provide a clear understanding of how schema elements are matched or merged. This transparency is crucial in critical applications where the rationale behind schema transformations must be auditable and understandable.

Rule-based methods can be categorized into two main types: those that use fixed, predefined rules and those that employ dynamically generated rules. Fixed rules are typically based on domain-specific heuristics, such as attribute name similarity or data type compatibility. For example, a rule might specify that two attributes with identical names but different data types should be considered as potential matches if they share a common domain context.

On the other hand, dynamically generated rules are often derived from machine learning models that analyze patterns in existing schema mappings. These models can learn complex relationships between schema elements and generate rules that capture these relationships. For instance, a machine learning algorithm might identify that attributes with similar distributions of values across different schemas are likely to be semantically related.

Rule-based methods also play a significant role in the refinement and optimization of schema matching and merging processes. Techniques such as constraint programming and rule optimization can be employed to ensure that the rules are both effective and efficient. Constraint programming, for example, can be used to derive rules from explicitly given constraints, ensuring that the resulting schema mappings satisfy certain consistency requirements.

In summary, rule-based methods offer a robust and interpretable approach to database schema matching and merging. By leveraging predefined or dynamically generated rules, these methods can effectively identify and reconcile schema discrepancies, providing a transparent and auditable process for schema transformations. This approach forms a solid foundation for more advanced machine learning-based techniques, which build upon the insights and structures established by rule-based methods.

### 2.2 Machine Learning-Based Approaches

Machine learning-based approaches have become increasingly prominent in the field of database schema matching and merging, offering sophisticated techniques to automate and enhance these processes. These methods leverage the power of algorithms to learn patterns and relationships within and between schemas, thereby improving the accuracy and efficiency of matching and merging tasks. One of the key advantages of machine learning in this context is its ability to handle the complexity and variability of schema structures, which traditional rule-based methods may struggle with.

Automated machine learning (AutoML) frameworks play a crucial role in schema matching by automating the process of feature engineering, model selection, and hyperparameter tuning. This automation reduces the need for extensive manual intervention, making the process more accessible to non-experts while still achieving high performance. Techniques like Bayesian optimization and reinforcement learning can be employed to explore the vast search space of possible schema mappings and identify the most effective ones.

In the context of schema merging, machine learning can be used to identify and resolve conflicts between schemas, such as differences in attribute names, data types, and relationships. By training on historical data of schema merges, machine learning models can learn to predict the most appropriate way to integrate schemas, thereby reducing the risk of data loss or inconsistency. Additionally, machine learning can enhance the explainability of schema matching and merging decisions, which is crucial for ensuring trust and compliance in data management processes.

Furthermore, the integration of machine learning with other techniques, such as evolutionary algorithms and deep learning, can lead to more robust and scalable solutions for schema matching and merging. For example, deep learning models can be used to capture complex, non-linear relationships between schema elements, while evolutionary algorithms can optimize the merging process by iteratively improving the schema integration strategy.

Overall, machine learning-based approaches offer a promising avenue for advancing the field of database schema matching and merging, providing more accurate, efficient, and scalable solutions. As these methods continue to evolve, they are likely to play an increasingly important role in the automation and optimization of data management tasks.

### 2.3 Probabilistic Approaches

Probabilistic approaches to database schema matching and merging leverage the principles of probability theory to address the inherent uncertainty and variability in schema structures. These methods often employ Bayesian inference, maximum likelihood estimation, and other probabilistic models to quantify the likelihood of different schema alignments and to merge schemas in a way that maximizes the probability of a correct match.

One of the key advantages of probabilistic approaches is their ability to handle the ambiguity and noise commonly found in real-world schemas. By assigning probabilities to different possible matches, these methods can provide a more nuanced and robust solution compared to deterministic techniques. For instance, Bayesian information criterion and other statistical criteria can be used to select the most appropriate model for schema matching, which can be viewed as maximizing a penalized likelihood function.

Another important aspect of probabilistic approaches is their ability to incorporate prior knowledge and update beliefs as new information becomes available. This is particularly useful in dynamic environments where schemas evolve over time. Bayesian probabilistic numerical methods, for example, allow for the incorporation of prior distributions and the updating of beliefs based on new data, making these methods adaptable to changing schema structures.

Probabilistic methods also offer a natural framework for dealing with incomplete or heterogeneous data. By modeling the uncertainty in the data, these methods can provide more reliable and interpretable results. For instance, probabilistic approaches can integrate qualitative and quantitative information in biological models, leading to a more accurate and comprehensive understanding of complex systems.

In summary, probabilistic approaches to database schema matching and merging offer a powerful and flexible framework for handling the complexities and uncertainties inherent in schema structures. By leveraging principles from probability theory and Bayesian inference, these methods can provide more robust, interpretable, and adaptive solutions to the challenges of schema alignment and integration.

### 2.4 Syntactic Matching Techniques

Syntactic matching techniques are pivotal in database schema matching and merging, focusing on structural similarities between schema elements. These techniques identify correspondences based on syntactic properties such as attribute names, data types, and structural relationships. Pattern matching algorithms are commonly used to detect syntactic similarities, enhanced by features like keyword frequency and syntactic patterns.

Syntactic parsers further analyze schema descriptions to identify syntactic dependencies and relationships indicative of semantic correspondences. These parsers can detect syntactic regularities across schemas, aiding in potential match inference. Integrating syntactic information with semantic and structural matching techniques enhances accuracy.

Syntactic transformations normalize schema descriptions, facilitating correspondence identification. These techniques extend to multilingual schemas by leveraging common syntactic patterns across languages. Overall, syntactic matching techniques provide a robust framework for accurate schema matching and merging by emphasizing syntactic properties.

### 2.5 Semantic Matching Techniques


Semantic matching techniques are essential for aligning and integrating heterogeneous data sources by identifying correspondences between semantically related elements. These techniques leverage various methods to capture the meaning and context of data elements, enabling more accurate and meaningful data integration.

One prominent approach is the use of semi-supervised learning frameworks, which combine supervised and unsupervised learning to improve semantic matching accuracy [83]. These frameworks often employ convolutional neural networks (CNNs) to estimate correspondences between semantically related images, leveraging cyclic consistency constraints on unlabeled image pairs to enhance model performance.

Another significant approach involves the application of semantic web matching to improve web service replacement [84]. This method utilizes bipartite graphs to match functionalities and capabilities between services advertised in ontology languages like OWL, ensuring seamless integration and interoperability.

Semantic matching is also applied in cross-lingual settings, where the challenge lies in aligning concepts across different languages [75]. This involves techniques that compare and match ontologies expressed in different natural languages, often using inter-lingual reference approaches to enhance interoperability [75].

In the context of text processing, semantic matching techniques are used to explore the extent to which a particular idea is expressed in a text collection [81]. This involves semantically matching natural language propositions against a corpus, which can be particularly useful in tasks like disaster recovery and emergency management.

Furthermore, semantic matching techniques are employed in information retrieval to improve relevance ranking by finding salient context based on semantic matching [82]. This approach involves locating the most salient context within documents and using semantic similarity between query terms and salient context terms to rank documents.

Overall, semantic matching techniques are vital for enhancing data integration, improving information retrieval, and enabling cross-lingual interoperability, making them a critical component in modern data management and analysis systems.


### 2.6 Hybrid Methods

Hybrid methods for database schema matching and merging represent a sophisticated approach that combines the strengths of multiple techniques to achieve more accurate and efficient schema integration. These methods leverage the complementary advantages of different algorithms, often integrating rule-based, statistical, and machine learning techniques to overcome the limitations of individual approaches.

One of the key advantages of hybrid methods is their ability to handle the complexity and heterogeneity of real-world schemas. For instance, rule-based methods can enforce domain-specific constraints and heuristics, while statistical methods identify patterns and correlations in the data. Machine learning techniques can then refine these patterns, improving the accuracy of schema matching. This combination allows hybrid methods to adapt to various scenarios, from simple schema structures to highly complex and dynamic environments [86].

Hybrid methods also enhance the robustness and scalability of schema matching and merging processes. By integrating multiple techniques, these methods can provide more reliable results even when dealing with large and diverse datasets. For example, a hybrid approach might start with a rule-based method to quickly identify obvious matches, followed by a statistical method to refine these matches, and finally, a machine learning model to predict and validate the final schema mappings [88].

Moreover, hybrid methods can be tailored to specific application domains and requirements. For instance, in bioinformatics, where data integration is critical, hybrid methods can combine genomic data with clinical information to create comprehensive and accurate schema mappings [85]. Similarly, in financial databases, hybrid methods can integrate transactional data with regulatory requirements to ensure compliance and data integrity [87].

In summary, hybrid methods for database schema matching and merging offer a powerful and flexible solution by integrating the strengths of various techniques. They provide a robust, scalable, and domain-specific approach to schema integration, making them indispensable in modern data management systems [86].

### 2.7 Crowdsourcing and Human-in-the-Loop Approaches

Crowdsourcing and human-in-the-loop (HITL) approaches have become integral in addressing the complexities of database schema matching and merging. These methods leverage the collective intelligence of human participants to solve tasks that are challenging for automated systems alone. Crowdsourcing platforms, such as Amazon Mechanical Turk, enable the distribution of tasks to a large, diverse group of individuals, who provide annotations, classifications, and other forms of input that can be aggregated to infer the correct schema mappings.

One of the key advantages of crowdsourcing is its ability to handle tasks that require nuanced understanding or domain-specific knowledge, which machines may lack. For instance, in schema matching, human workers can identify subtle semantic differences between database attributes that automated algorithms might miss [92]. However, the involvement of humans introduces challenges such as variability in the quality of responses and the need for efficient task allocation and quality control mechanisms.

Human-in-the-loop systems integrate human expertise into the machine learning pipeline, allowing for iterative improvements and corrections. This approach is particularly useful in schema merging, where human oversight can ensure that the merged schema retains the necessary semantic integrity and usability [93]. HITL systems often employ adaptive algorithms that learn from human feedback, continuously refining the schema matching and merging process.

Quality control in crowdsourcing is a critical aspect, and various techniques have been developed to assess and improve the reliability of human contributions. These include pre-screening workers, using redundancy (asking multiple workers the same question), and employing statistical models to infer the true labels from noisy crowdsourced labels [90]. Additionally, incentive mechanisms can be designed to motivate high-quality contributions, ensuring that the crowd remains engaged and productive [89].

Despite the benefits, crowdsourcing and HITL approaches also face challenges such as scalability, cost, and the need for robust mechanisms to handle noisy or malicious inputs. Future research in this area should focus on developing more efficient algorithms for task allocation, improving the accuracy of label aggregation, and exploring hybrid models that combine the strengths of both human and machine intelligence [91].

In summary, crowdsourcing and human-in-the-loop approaches offer powerful tools for database schema matching and merging by harnessing the collective intelligence of humans. However, they require careful design and management to ensure high-quality outcomes and efficient use of resources. This integration of human and machine intelligence complements the strengths of deep learning and neural network-based methods, providing a comprehensive framework for tackling the complexities of schema integration.

### 2.8 Deep Learning and Neural Network-Based Methods

Deep learning and neural network-based methods have emerged as powerful tools for database schema matching and merging, leveraging their ability to learn complex patterns and relationships from data. These methods can be particularly effective in scenarios where traditional rule-based or heuristic approaches fall short due to the complexity and variability of schema structures.

One of the key advantages of deep learning in this context is its ability to handle large and diverse datasets, which is often a requirement in schema matching tasks. For instance, deep neural networks can be trained to identify similarities and differences between schemas by learning feature representations that capture the underlying structure of the data [96]. This capability is particularly useful in environments where schemas evolve over time or where there is a need to integrate heterogeneous data sources.

Moreover, deep learning techniques can be applied to automate the merging of schemas by learning to align and combine different schema elements. This can be achieved through various architectures, such as autoencoders [97], which can be used to learn compressed representations of schemas that facilitate matching and merging operations. Additionally, graph neural networks (GNNs) [94] can be employed to model the relationships between schema elements as a graph, allowing for more sophisticated and context-aware matching and merging strategies.

Another promising approach is the use of generative models, such as generative adversarial networks (GANs) [97], which can be trained to generate synthetic schemas that can be used to augment the training data for schema matching and merging tasks. This can help improve the robustness and generalization capabilities of deep learning models in handling unseen or rare schema configurations.

Despite their potential, deep learning-based methods for schema matching and merging also present challenges, such as the need for large amounts of labeled data for training and the difficulty in interpreting the decisions made by these models. However, recent advancements in explainable AI [98] and model interpretability [95] are addressing these issues, providing insights into how deep learning models arrive at their conclusions and making them more transparent and trustworthy.

In summary, deep learning and neural network-based methods offer a robust and flexible framework for tackling the complexities of database schema matching and merging. By leveraging their ability to learn from data and model complex relationships, these methods can significantly enhance the accuracy and efficiency of schema integration processes [96], [97], [94]. This integration complements the strengths of crowdsourcing and human-in-the-loop approaches, providing a comprehensive framework for tackling the complexities of schema integration.

### 2.9 Large Language Models (LLMs) in Schema Matching


Large Language Models (LLMs) have emerged as powerful tools for schema matching and merging, leveraging their advanced semantic understanding and contextual capabilities to identify correspondences between elements of disparate schemas. Recent studies have demonstrated the potential of LLMs in this domain, particularly in scenarios where traditional methods based on string similarity or rule-based systems fall short [4]. LLMs can infer semantic relationships between schema elements using only their names and descriptions, thereby reducing the need for extensive data instances or manual intervention [4].

One of the key advantages of using LLMs for schema matching is their ability to generalize across different domains and contexts. For instance, LLMs have been successfully applied to ontology matching, where they have shown performance on par with or even surpassing traditional systems, especially in complex scenarios [99]. This capability is particularly valuable in large enterprises where schema matching is often a precursor to human decision-making rather than just code generation [2].

However, the application of LLMs in schema matching is not without challenges. The quality of matching can be influenced by the amount of context provided in the prompts, with too little or too much context potentially degrading performance [4]. Additionally, the robustness of LLMs to out-of-distribution entities remains an area of active research [101]. Fine-tuning LLMs for specific tasks can improve their performance, but it also introduces complexities related to generalization and cross-domain transfer [100].

Despite these challenges, LLMs offer a promising avenue for bootstrapping the schema matching process. Their ability to assist data engineers in speeding up this task, solely based on schema element names and descriptions, underscores their potential in enhancing data integration and interoperability [4]. Future research should focus on optimizing prompt design, improving generalization capabilities, and exploring hybrid approaches that combine LLMs with traditional schema matching techniques to leverage the strengths of both methodologies.

In summary, LLMs provide a robust framework for schema matching and merging, offering advanced semantic understanding and contextual capabilities that can significantly enhance the accuracy and efficiency of schema integration processes. By addressing current challenges and exploring hybrid approaches, LLMs can further solidify their role in modern data management.


### 2.10 Self-Improving and Adaptive Methods

Self-improving and adaptive methods in database schema matching and merging represent a significant advancement in the field, enabling systems to dynamically enhance their performance and accuracy over time. These methods leverage the principles of self-adaptation and self-improvement, which are central to the concept of Organic Computing [104]. By incorporating self-* properties, such as self-improvement, these systems can not only adapt to changing environments but also refine their adaptation logic to optimize overall system performance.

One of the key strategies in self-improving algorithms is the ability to fine-tune themselves based on unknown input distributions [102]. This approach allows algorithms to process sequences of inputs with varying distributions, ultimately achieving optimal expected complexity. For instance, self-improving algorithms for sorting and Delaunay triangulation begin with a training phase to gather information about the input distribution, followed by a stationary phase where the algorithms operate at their optimized state [102].

The generalization of self-improving algorithms extends this concept to handle dependencies among input instances under group product distributions [106]. This model allows for more complex interactions between data elements, enhancing the adaptability and robustness of the algorithms.

In the context of database schema matching and merging, self-improving methods can be applied to enhance the accuracy and efficiency of schema alignment processes. By continuously learning from new data and refining their matching and merging strategies, these systems can improve their performance over time. For example, machine learning techniques can be used to update adaptation rules and policies, ensuring that the system maintains high-quality attributes even as the environment changes [105].

Moreover, the integration of self-adaptive training can bridge supervised and self-supervised learning, improving the generalization of deep neural networks under various levels of noise [103]. This capability is particularly valuable in database schema matching, where the quality of data can vary significantly.

In summary, self-improving and adaptive methods in database schema matching and merging offer a promising avenue for enhancing system performance through continuous learning and adaptation. By leveraging principles from Organic Computing and machine learning, these methods can ensure that database systems remain efficient and accurate in dynamic and uncertain environments.

## 3 Applications and Use Cases of Schema Matching and Merging

### 3.1 Data Integration in Large Enterprises

Data integration in large enterprises is a critical challenge due to the sheer volume and diversity of data sources, each with its own schema and structure. The integration process often involves schema matching and merging to create a unified view of the data, which is essential for effective decision-making and operational efficiency. Techniques such as batch and incremental schema integration algorithms, as presented in [1], address the representation differences in attribute names across various data sources, using ED Join and Semantic Join algorithms to ensure efficient and effective integration.

In the context of large enterprises, data source selection is a pivotal aspect of data integration. [107] emphasizes the importance of considering both efficiency and effectiveness in data source selection, as accessing all data sources can be costly and impractical. The paper proposes methods that can scale to datasets with millions of data sources while ensuring the quality of integrated data.

The integration of heterogeneous data sources is further complicated by the need to account for different types of data, such as numerical, categorical, and textual data. [58] introduces a stratified data integration approach that organizes data into independent representation layers, addressing semantic heterogeneity uniformly within each layer. This approach facilitates the integration of complex data types and enhances the overall quality of the integrated data.

Moreover, the integration process must often handle high-dimensional data from multiple sources. [108] proposes a novel sparse latent factor regression model to integrate heterogeneous data, providing a tool for dimensionality reduction while correcting for batch effects. This model is particularly useful in bioinformatics applications, where data quality and consistency are paramount.

In summary, data integration in large enterprises requires sophisticated methods to handle the complexity and scale of data sources. Techniques such as schema integration, data source selection, stratified data integration, and high-dimensional data integration are essential for creating a unified and accurate view of enterprise data. These methods not only improve the efficiency of data integration but also enhance the quality and reliability of the integrated data, ultimately supporting better decision-making and operational outcomes.

### 3.2 Semantic Web and Linked Data Integration


The integration of Linked Data and Semantic Web technologies has become increasingly important for various applications, particularly in domains requiring the seamless merging of heterogeneous data sources. One notable area is the life sciences, where the integration of biological data from diverse sources is crucial for advancing research. Semantic Web technologies provide a robust framework for sharing and reusing data, but their application in life sciences presents unique challenges due to the complexity and specificity of biological data.

In the context of the Semantic Web, the integration of procedural knowledge, or "know-how," has been explored to enhance the interoperability and usability of web-based information. This approach extends the benefits of Linked Data to procedural knowledge, enabling the automatic acquisition and linking of step-by-step instructions with other online knowledge bases. Such integration is particularly valuable in domains like healthcare, where clinical workflows and decision-making processes can be enriched with linked procedural knowledge.

The development of query languages for the Web of Linked Data has also seen significant advancements, with languages like LDQL offering novel ways to query distributed data sources. These languages separate the description of query results from the selection of data sources, enhancing the expressiveness and flexibility of queries. This capability is essential for large-scale data integration projects, where the ability to efficiently query and navigate through vast amounts of linked data is critical.

Furthermore, the application of semantic technologies to digital content exposed as Linked Data has facilitated the exploration and selection of resources based on semantic similarity. This approach has been particularly useful in environmental research, where the comparison of datasets annotated with terminologies from Linked Data sources can aid in resource selection and discovery.

Overall, the integration of Linked Data and Semantic Web technologies offers promising solutions for data integration across various domains, enhancing the interoperability and usability of distributed data sources. This is particularly relevant in collaborative environments and service integration, where seamless interoperability and data consistency are essential for dynamic and evolving networks.


### 3.3 Collaborative Environments and Service Integration

In collaborative environments and service integration, the methods of database schema matching and merging are essential for ensuring seamless interoperability and data consistency across diverse systems. These methods are crucial for integrating heterogeneous data sources within collaborative networks, where multiple stakeholders contribute and access shared data. The integration process involves identifying and reconciling differences in schema structures, data types, and naming conventions across various databases. This is particularly important in service-oriented architectures (SOA) where services need to interact dynamically and efficiently.

One of the key challenges in collaborative environments is the management of data interoperability, which requires robust schema matching techniques to identify correspondences between different schemas. This is often achieved through semantic analysis and machine learning algorithms that can infer relationships between schema elements based on their descriptions and usage patterns. Once correspondences are established, schema merging techniques are employed to create a unified schema that preserves the integrity and usability of the original data sources.

In service integration, the ability to dynamically adapt and integrate services is critical for supporting the evolving needs of collaborative networks. This involves not only the integration of data schemas but also the alignment of service interfaces and workflows to ensure that services can be composed and orchestrated effectively. The integration process must be agile and responsive to changes in the environment, such as the addition of new services or the modification of existing ones.

Moreover, the integration of services in collaborative environments often requires the mediation of information systems to facilitate the exchange of data and processes between different stakeholders. This mediation is typically supported by a service-based platform that provides mechanisms for service discovery, composition, and execution. The platform must ensure that the integrated services adhere to the agreed-upon standards and protocols, thereby maintaining the consistency and reliability of the collaborative network.

In summary, the methods of database schema matching and merging are fundamental to the successful integration of services in collaborative environments. They enable the seamless exchange of data and processes across heterogeneous systems, supporting the dynamic and evolving nature of collaborative networks. By addressing the challenges of schema heterogeneity and service interoperability, these methods contribute to the creation of robust and flexible collaborative environments that can adapt to changing requirements and support diverse use cases.

### 3.4 Healthcare and Bioinformatics

In the realm of healthcare and bioinformatics, the integration of database schema matching and merging techniques has become increasingly crucial. These methods facilitate the harmonization of diverse data sources, enabling more comprehensive and accurate analyses. For instance, in bioinformatics, the need to manage and interpret vast amounts of genomic data has led to the development of sophisticated schema matching algorithms that can align and merge datasets from various genomic databases. This is particularly important in structural bioinformatics, where computational techniques are used to analyze protein structures. The ability to match and merge schemas from different structural databases ensures that researchers can access a unified view of protein structures, enhancing the accuracy and efficiency of computational analyses.

Moreover, the advent of deep learning and machine learning in bioinformatics has further underscored the importance of schema matching and merging. These technologies require large, diverse datasets to train models effectively. However, healthcare data is often fragmented across different institutions, making it challenging to create robust models that generalize across populations. Federated learning, which allows for the training of shared models while keeping sensitive data local, is one approach that leverages schema matching and merging to connect fragmented data sources while preserving privacy.

In clinical settings, the use of large language models (LLMs) like GPT-3.5 and GPT-4 has shown promise in supporting real-world information needs. However, the effectiveness of these models relies on the quality and consistency of the underlying data schemas. Schema matching and merging techniques ensure that LLMs can access and process data from heterogeneous sources, thereby improving their performance in clinical decision support and other healthcare applications.

Furthermore, the integration of information technology in healthcare has introduced new challenges and opportunities for schema matching and merging. Electronic health records (EHRs), telemedicine platforms, and wearable devices generate vast amounts of data that need to be harmonized to provide meaningful insights. Effective schema matching and merging are essential for ensuring that these data sources can be integrated and analyzed to improve patient outcomes and healthcare delivery.

In summary, the application of schema matching and merging techniques in healthcare and bioinformatics is pivotal for managing the complexity of diverse data sources, enhancing the accuracy of computational analyses, and supporting the development of advanced machine learning models. As the field continues to evolve, these methods will play an increasingly critical role in unlocking the full potential of healthcare data.

### 3.5 Financial and E-commerce Applications

In the realm of financial and e-commerce applications, database schema matching and merging are essential for ensuring data integrity, interoperability, and efficiency. These techniques are particularly crucial in environments where data from multiple sources need to be integrated, such as in financial institutions that manage diverse portfolios or e-commerce platforms that aggregate product information from various vendors.

For financial applications, schema matching and merging are vital for consolidating data from different accounting systems, customer databases, and transaction logs. This integration is crucial for generating comprehensive financial reports, conducting risk assessments, and ensuring regulatory compliance. For instance, the ability to match and merge schemas from various banking systems can streamline the process of generating consolidated financial statements, which is a critical requirement for both internal decision-making and external reporting [128].

In e-commerce, schema matching and merging are equally important for managing product catalogs, customer profiles, and transaction histories. E-commerce platforms often need to integrate data from multiple suppliers, each with its own schema, to provide a unified shopping experience. This integration ensures that the data is consistent and accurate across different systems. For example, a large e-commerce platform might need to merge product schemas from different vendors to create a comprehensive catalog that includes all relevant product details, such as pricing, availability, and customer reviews [126].

The advent of Web 3.0 technologies, such as blockchain and decentralized storage, introduces new challenges and opportunities for schema matching and merging. These technologies promise enhanced security and privacy, which are critical in financial and e-commerce applications. However, they also require sophisticated data management techniques to ensure that data from decentralized sources can be effectively integrated and utilized [127].

In summary, the methods of database schema matching and merging are indispensable in financial and e-commerce applications. They enable the seamless integration of data from diverse sources, ensuring data consistency, accuracy, and accessibility. As these industries continue to evolve, particularly with the rise of Web 3.0 technologies, the importance of robust schema matching and merging techniques will only grow [128].

### 3.6 Public Data Integration and Open Data

Public data integration and open data initiatives have become pivotal in modern data management, offering significant opportunities for data sharing, collaboration, and innovation. The integration of public data, often sourced from various government agencies, research institutions, and open data portals, presents unique challenges and opportunities for schema matching and merging. These initiatives aim to enhance data accessibility, interoperability, and usability, thereby fostering a more transparent and efficient data ecosystem.

One notable application is the development of data-aware chatbots and natural language interfaces, which facilitate user interaction with open data portals [135]. These interfaces lower the barrier to entry for non-technical users, making open data more accessible and engaging. Similarly, projects like Blended Integrated Open Data (BIOD) aim to simplify access to large volumes of public open data by integrating disparate datasets and providing query capabilities [136]. This approach not only reduces the effort required for data integration but also enhances the potential for data-driven insights and decision-making.

The ethical and quality considerations of open data are also critical. Robust data quality management solutions are necessary to ensure the trustworthiness and reliability of open data [134]. Additionally, ethical considerations in open data, such as privacy and fairness, must be addressed to maintain public trust and compliance with regulations [132].

Technological advancements, such as ontology-based data integration and linked open data (LOD) initiatives, further enhance the potential of public data integration [131], [130], [129]. These approaches enable semantic interoperability, allowing data from different sources to be linked and queried in a unified manner. For instance, the integration of COVID-19 data from multiple countries demonstrates the potential of ontology-based approaches to facilitate large-scale data integration and analysis [131].

Moreover, the integration of public and private data, while ensuring privacy through differential privacy techniques, offers a balanced approach to leveraging data for statistical analysis and decision-making [133]. This approach allows for the use of public data to enhance the accuracy and utility of private data analysis, without compromising individual privacy.

In conclusion, public data integration and open data initiatives are transforming the landscape of data management and utilization. By addressing challenges related to data quality, privacy, and interoperability, these initiatives are paving the way for more informed, transparent, and collaborative data ecosystems.

### 3.7 Machine Learning and AI Data Preparation


In the context of machine learning and AI data preparation, ensuring data quality, representativity, and the integration of knowledge are paramount. Industrial applications, such as those in steel production, often face issues with missing data due to sensor unavailability [137]. Techniques for imputing missing values or utilizing partially available data are crucial for maintaining data integrity and ensuring that models can be trained effectively.

Data representativity is another critical aspect, as it ensures that the data used for training AI systems accurately reflects the target population [141]. This is particularly important in fields like healthcare and social sciences, where biased or unrepresentative data can lead to ineffective or even harmful AI models. The development of metrics and frameworks to assess data representativity, such as the AI Data Readiness Inspector (AIDRIN), helps in evaluating and improving data quality for AI applications [138].

The integration of knowledge and data is also a significant area of focus. Knowledge embedding and knowledge discovery methods aim to combine domain-specific knowledge with data-driven approaches to enhance model robustness and accuracy [140]. This dual approach not only improves the performance of AI models but also helps in uncovering new scientific principles and enhancing the interpretability of AI systems.

Moreover, the rapid development and deployment of AI models require efficient data processing frameworks. Tools like tf.data provide a robust infrastructure for building and executing efficient input pipelines, ensuring that data processing does not become a bottleneck in the training process [139]. These frameworks are essential for handling the large volumes of data required by modern AI models, especially in deep learning applications.

In summary, the preparation of data for machine learning and AI involves addressing issues of data quality, representativity, and the integration of knowledge. Techniques for handling missing data, assessing data representativity, and efficient data processing frameworks are all critical components in ensuring that AI models are effective, reliable, and capable of delivering meaningful insights.


### 3.8 Real-time Data Integration and Streaming Applications

Real-time data integration and streaming applications are critical for handling the dynamic and continuous nature of data streams in various domains. These applications require efficient methods for schema matching and merging to ensure data consistency and interoperability across different sources. In real-time streaming scenarios, the ability to process data as it arrives is essential for applications such as financial trading, IoT sensor data analysis, and autonomous driving.

One approach to real-time data integration is the use of weighted reservoir sampling for approximate query processing on streaming data [145]. This method allows for the approximation of OLAP queries on data streams, enabling efficient processing of large volumes of data without the need to store the entire stream. For graph-based data streams, community detection algorithms can be used to identify large connected components, which can then be correlated across different streams to facilitate integration [145].

Another important aspect is the integration of streaming services with other data transmission protocols, such as TCP, to optimize bandwidth usage and ensure timely data delivery [143]. This involves managing the allocation of resources to different types of traffic, such as elastic and streaming data, to balance the demands of real-time processing with the need for reliable data transmission.

For applications requiring hard real-time guarantees, such as adaptive streaming in real-time systems, the Mode Aware Data Flow (MADF) model can be employed to capture the behavior of streaming applications and ensure predictable performance [142]. This model allows for efficient temporal analysis during mode transitions, ensuring that the system remains predictable and meets its timing constraints.

In the context of real-time multimedia streaming, optimizing network traffic management is crucial to maintain a high-quality user experience [144]. This involves managing the quality of service (QoS) metrics, such as latency and throughput, to ensure that streaming data is delivered efficiently and without interruptions.

Overall, the integration of schema matching and merging techniques with real-time data processing frameworks is essential for enabling efficient and reliable data integration in streaming applications. By leveraging advanced algorithms and models, such as weighted reservoir sampling, community detection, and mode-aware data flow, these applications can handle the challenges of real-time data integration and provide timely and accurate insights from continuous data streams. This seamless integration is crucial for maintaining data integrity and system functionality as database systems evolve and adapt to new requirements.

### 3.9 Schema Evolution and Legacy System Integration

Schema evolution and legacy system integration are critical challenges in maintaining the integrity and functionality of database systems as they evolve over time. The integration of legacy systems, which often have rigid and outdated schemas, with newer systems requires careful management of schema changes to ensure data consistency and system interoperability. This subsection explores the methods and applications of schema matching and merging in the context of schema evolution and legacy system integration.

Schema evolution involves the continuous adaptation of database schemas to accommodate new requirements, technological advancements, and changes in business needs. This process is essential for maintaining the relevance and efficiency of database systems. However, it also introduces complexities, such as ensuring backward compatibility, managing data migration, and preserving data integrity during schema changes [147]. Techniques such as schema versioning, schema mapping, and data transformation are employed to facilitate smooth transitions between schema versions [149].

Legacy system integration focuses on incorporating older, often monolithic systems into modern, distributed architectures. These legacy systems are characterized by their rigid schemas, which can pose significant challenges during integration efforts. The integration process often involves schema matching to identify correspondences between the legacy schema and the new system's schema, followed by schema merging to create a unified schema that accommodates both systems [146]. This process requires careful consideration of data semantics, structural differences, and potential conflicts between the schemas [1].

One of the key challenges in schema evolution and legacy system integration is ensuring that changes to the schema do not disrupt existing applications or data workflows. This requires the development of robust schema evolution strategies that can handle various types of schema modifications, such as adding or removing columns, changing data types, and restructuring tables [147]. Additionally, the integration of legacy systems often necessitates the use of middleware and transformation layers to bridge the gap between the old and new systems, ensuring seamless data flow and interoperability [148].

In conclusion, schema evolution and legacy system integration are critical aspects of maintaining the functionality and relevance of database systems. Effective management of these processes requires a combination of advanced schema matching and merging techniques, robust schema evolution strategies, and careful consideration of data semantics and system interoperability [1]. By addressing these challenges, organizations can ensure the smooth transition of their database systems, preserving data integrity and system functionality as they evolve.

### 3.10 Human-in-the-Loop Schema Matching

Human-in-the-loop (HITL) schema matching represents a paradigm shift in data integration, where human expertise is integrated into the automated process to enhance the accuracy and relevance of schema correspondences. This approach leverages the strengths of both human intuition and machine learning algorithms to overcome the limitations of purely automated methods. For instance, [151] demonstrates a system that uses GPT-3 to generate schematic elements, which are then manually edited by humans to refine the schema graph. This interactive process not only improves the transferability of the schema to new domains but also reduces the effort required for human curation.

In the context of schema matching, human involvement can significantly improve the quality of matches, especially in scenarios where semantic heterogeneity and complex data structures pose challenges. [152] introduces PoWareMatch, a deep learning-based system that calibrates and filters human matching decisions to generate high-quality matches. This approach recognizes the inherent biases in human decisions and uses machine learning to mitigate them, resulting in more accurate schema correspondences.

Moreover, HITL schema matching can be particularly beneficial in domains where data quality and interpretability are critical. [153] presents a framework for human-in-the-loop construction of schema libraries, where non-experts can interactively refine event schemas. This method ensures that the final schema library is both comprehensive and aligned with human understanding, thereby enhancing its utility in real-world applications.

The integration of human feedback in schema matching also addresses the issue of model interpretability and trustworthiness. [150] discusses the challenges and opportunities in accelerating HITL machine learning, emphasizing the need for systems that can rapidly iterate and adapt based on human input. This iterative process allows for continuous improvement of schema matching algorithms, ensuring that they remain robust and reliable as data sources evolve.

In summary, HITL schema matching combines the precision of human judgment with the scalability of machine learning, offering a balanced approach to data integration. By incorporating human expertise at critical stages of the matching process, HITL systems can achieve higher accuracy, better interpretability, and greater adaptability to diverse and complex data environments. This approach is particularly valuable in schema evolution and legacy system integration, where the need for precise and adaptable schema management is paramount.

## 4 Evaluation and Performance Metrics

### 4.1 Precision and Recall in Schema Matching

Precision and recall are fundamental performance metrics in schema matching, providing insights into the accuracy and completeness of matching results. Precision measures the proportion of correctly identified matches among all identified matches, while recall quantifies the proportion of actual matches that were correctly identified. These metrics are crucial for evaluating the effectiveness of schema matching algorithms, as they offer a balanced view of both false positives and false negatives.

In schema matching, precision is particularly important because it indicates how reliable the matching results are. High precision means that the matches proposed by the algorithm are likely to be correct, reducing the effort required for manual verification. On the other hand, recall is essential for ensuring that no potential matches are missed, which is critical in scenarios where completeness of matches is paramount, such as data integration tasks.

Various approaches have been proposed to improve precision and recall in schema matching. For instance, hybrid methods that combine linguistic and data-driven techniques have been shown to achieve high F-scores by leveraging a global dictionary [154]. Similarly, deep learning mechanisms have been employed to calibrate human matching decisions, enhancing both precision and recall by generating high-quality matches [152].

Recent advancements in machine learning and natural language processing have also contributed to the improvement of these metrics. Large language models (LLMs) have been explored for schema matching, demonstrating that LLMs can assist in identifying semantic correspondences with high precision and recall, even in zero-shot scenarios [4]. Additionally, retrieval-enhanced LLM methods have been developed that eliminate the need for predefined mappings, achieving effective schema matching without access to source data [155].

Crowdsourcing has been another avenue for enhancing schema matching performance. By leveraging the collective intelligence of multiple workers, crowdsourcing can reduce uncertainty in schema matching, significantly impacting recall and precision [24]. This approach improves the overall quality of matching results by combining the accuracy rates of individual crowd workers.

In summary, precision and recall are indispensable metrics for assessing the performance of schema matching algorithms. By employing hybrid methods, leveraging advanced machine learning techniques, and utilizing crowdsourcing, researchers have made significant strides in improving both precision and recall, thereby enhancing the overall effectiveness of schema matching in data integration tasks.

### 4.2 F-Measure as a Comprehensive Metric

The F-measure, also known as the F1-score, is a widely used metric in the evaluation of classification algorithms, particularly in scenarios with imbalanced datasets. It combines precision and recall into a single metric, providing a balanced assessment of a classifier's performance. The F-measure is defined as the harmonic mean of precision and recall, given by the formula \( F = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}} \). This metric is particularly useful when the cost of false positives and false negatives is not uniform, as it balances the trade-off between these two types of errors.

One of the key advantages of the F-measure is its ability to provide a single, interpretable score that summarizes the performance of a classifier. This is particularly important in applications where both precision and recall are critical, such as in medical diagnosis or fraud detection. However, the F-measure has been criticized for its lack of intuitive interpretation and for combining two conceptually distinct aspects of performance. To address these concerns, researchers have proposed transformations of the F-measure, such as the F* (F-star) metric [157], which offers a more immediate practical interpretation.

Statistical formulas for the F-measure, including standard error and confidence intervals, have been developed to aid in sample size planning and to achieve accurate estimation of F-measures [158]. These formulas are based on the property of asymptotic normality in the large sample limit, providing a robust framework for evaluating the reliability of F-measure estimates.

In the context of optimizing F-measures, two primary approaches have been explored: empirical utility maximization (EUM) and decision-theoretic approaches [156]. The EUM approach focuses on learning a classifier that performs optimally on the training data, while the decision-theoretic approach involves learning a probabilistic model and then predicting labels with maximum expected F-measure. Empirical studies suggest that while both approaches are asymptotically equivalent given large datasets, the EUM approach tends to be more robust against model misspecification, and the decision-theoretic approach is better suited for handling rare classes and domain adaptation scenarios.

Overall, the F-measure remains a comprehensive and widely applicable metric for evaluating classification performance, particularly in scenarios where precision and recall need to be balanced. Its versatility and interpretability make it a valuable tool in the arsenal of performance metrics for machine learning and data mining applications. This metric is particularly relevant in the context of schema matching, where both the accuracy of identified matches (precision) and the completeness of the matching process (recall) are critical. By leveraging the F-measure, researchers and practitioners can better assess and optimize the performance of schema matching algorithms, ensuring that they meet the demands of large-scale and complex data integration tasks.

### 4.3 Scalability and Performance in Large-Scale Databases

Scalability and performance in large-scale databases are critical factors that determine the efficiency and effectiveness of database schema matching and merging processes. As databases grow in size and complexity, the ability to handle these operations without compromising performance becomes increasingly challenging. Scalability refers to the system's ability to handle increasing amounts of data and user load without a proportional decrease in performance. Performance, on the other hand, measures the speed and efficiency with which these operations can be executed.

In large-scale databases, scalability is often addressed through distributed systems and parallel processing techniques. For instance, [160] discusses the scalability of multidimensional databases, emphasizing the importance of compression techniques to manage rapid data growth. Similarly, [163] compares the performance and scalability of relational and document-based web systems, finding that non-relational solutions can be more efficient for certain applications.

Performance in large-scale databases is influenced by various factors, including the choice of database management system (DBMS), indexing strategies, and query optimization techniques. [159] introduces a dynamic programming algorithm for computing sufficient statistics in large databases, demonstrating how efficient algorithms can enhance performance. Additionally, [161] presents a scalable blocking algorithm for very large databases, highlighting the importance of minimizing data movement and using approximate counting structures to maintain performance.

The impact of hardware and software configurations on performance is also significant. [162] investigates the performance characteristics of GPUs and CPUs for database analytics, showing that while GPUs can offer substantial speedups, the performance gains are not always proportional to the hardware capabilities. Furthermore, [164] examines the performance and scalability of distributed database systems in hybrid clouds, revealing that factors such as network distance can significantly affect throughput and scalability.

In summary, scalability and performance in large-scale databases require careful consideration of both algorithmic and architectural choices. By leveraging advanced techniques and understanding the interplay between different components, it is possible to design and implement database schema matching and merging processes that are both scalable and performant. This understanding is crucial for selecting appropriate performance measures that align with the specific goals of schema matching and merging tasks, as discussed in the following subsection.

### 4.4 Comparative Analysis of Performance Measures

The comparative analysis of performance measures is essential for evaluating the effectiveness of database schema matching and merging techniques. This subsection provides a comprehensive overview of various performance measures used in the literature, highlighting their relative strengths and weaknesses. The selection of appropriate performance measures is crucial for accurately assessing the performance of schema matching and merging algorithms, as different measures may emphasize different aspects of the problem, such as precision, recall, accuracy, or computational efficiency [168].

One of the key challenges in this domain is choosing a measure that aligns with the specific goals of the schema matching or merging task. For instance, in scenarios where the primary concern is the overall success rate of matching or merging, measures like accuracy and precision may be more suitable [168]. However, in cases where the dataset is imbalanced or where specific classification requirements are present, non-decomposable measures such as the F-measure may be more appropriate [165].

The comparative analysis also emphasizes the importance of understanding the theoretical properties of performance measures. For example, some measures may be equivalent under certain conditions, while others may lead to interpretation problems if not used carefully [168]. Additionally, the behavior of performance measures can vary significantly across different problem domains and datasets, necessitating a flexible approach to measure selection [167].

Moreover, the comparative analysis underscores the need for a unified framework to evaluate performance measures, especially in multi-label classification scenarios where traditional measures may fall short [166]. The use of margin-based views and max-margin approaches can provide a more robust evaluation framework, ensuring that the chosen measure aligns with the underlying objectives of the task [166].

In conclusion, the comparative analysis of performance measures in database schema matching and merging is crucial for selecting the most appropriate measure that aligns with the specific requirements of the task. By understanding the theoretical properties and practical implications of different performance measures, researchers and practitioners can make informed decisions that enhance the effectiveness and reliability of schema matching and merging techniques. This understanding is particularly important when integrating human expertise into the process, as discussed in the following subsection.

### 4.5 Human-in-the-Loop Evaluation Metrics

Human-in-the-loop (HITL) evaluation metrics are essential for assessing the performance of database schema matching and merging systems, especially when human expertise is integrated into the process. These metrics aim to quantify the effectiveness of human interventions in improving the accuracy and reliability of schema matching and merging tasks. The integration of human knowledge can significantly enhance the quality of the final schema, particularly in complex and ambiguous scenarios where automated systems may struggle.

One of the primary goals of HITL evaluation metrics is to measure the reduction in errors and the improvement in schema coherence resulting from human interventions. This can be achieved by comparing the performance of automated systems with and without human input, using metrics such as precision, recall, and F1-score [169]. Additionally, metrics like the Preference Identification Ratio (PIR) can be used to quantify the capacity of an evaluation metric to capture users' preferences, providing a more nuanced understanding of human-in-the-loop effectiveness [172].

Another important aspect of HITL evaluation is the assessment of the human cognitive load and the efficiency of the feedback loop. Metrics such as mental effort, task time, and user satisfaction can be used to evaluate how well the system supports human decision-making without overwhelming the user [170]. For instance, a system that minimizes the cognitive load while effectively incorporating human feedback is likely to be more efficient and user-friendly.

Moreover, the scalability of human evaluation is crucial for broader adoption. Dynamic approaches to human evaluation, such as those proposed in [171], can help determine the required number of human annotations for reliable model comparisons, thereby reducing the cost and time associated with extensive human evaluations. This dynamic approach ensures that the evaluation process is both efficient and effective.

In summary, HITL evaluation metrics for database schema matching and merging should focus on quantifying the improvement in schema quality, assessing the cognitive load on human users, and ensuring the scalability of the evaluation process. By incorporating these metrics, researchers and practitioners can design more effective and user-friendly HITL systems, ultimately leading to better database schema integration and management.

### 4.6 Data Separability and Its Impact on Performance

Data separability is a critical factor that significantly influences the performance of database schema matching and merging processes. It refers to the degree to which data can be easily distinguished or classified based on certain attributes or features. High data separability implies that the schemas can be accurately and efficiently matched, while low separability can lead to errors and increased computational complexity [178].

The impact of data separability on performance is multifaceted. High separability enhances the accuracy and speed of schema matching algorithms, as the data points are more distinct and easier to categorize [174]. Conversely, low separability results in higher computational costs and reduced accuracy, particularly in complex and multimodal data scenarios [177]. This challenge is especially relevant when traditional matching techniques struggle to differentiate between similar data points [175].

Several measures have been proposed to quantify data separability, such as the Distance-based Separability Index (DSI) [174] and the Rate of Separability (RS) [178]. These measures provide a quantitative assessment of how well data points from different classes can be distinguished. By incorporating these separability measures into the evaluation of schema matching algorithms, researchers can better understand the conditions under which these algorithms perform optimally [173].

Moreover, the separability of data influences the choice of matching and merging strategies. For instance, in cases where data is highly separable, simpler algorithms may suffice, whereas more complex and computationally intensive methods may be necessary for less separable data [176]. This trade-off between simplicity and complexity is a critical consideration in optimizing the performance of schema matching and merging processes.

In summary, data separability is a fundamental aspect that significantly impacts the performance of database schema matching and merging. By understanding and quantifying separability, researchers can develop more effective algorithms and strategies that adapt to the inherent characteristics of the data, thereby improving overall performance and accuracy [1, 2]. This understanding is crucial as we transition to more advanced generative models and large language models in schema matching, which require robust evaluation metrics to ensure their effectiveness in real-world applications.

### 4.7 Generative Models and Schema Matching Evaluation

Generative models have emerged as a pivotal tool in the evaluation of schema matching and merging tasks due to their ability to capture complex data distributions and generate synthetic data for testing and validation. However, evaluating generative models in schema matching presents unique challenges and opportunities. One of the primary challenges is the lack of standardized metrics that can effectively capture the nuances of schema matching performance. Traditional metrics like accuracy, precision, and recall often fall short in assessing the quality of matches, especially in complex, multi-domain scenarios.

Recent advancements in generative models, such as those leveraging pre-trained language models and generative large language models, have introduced novel approaches to schema matching. These models can generate synthetic datasets that mimic real-world data distributions, enabling more robust evaluation of schema matching algorithms. For instance, the use of "generative tags" enhances the effectiveness of schema matching by providing context-rich annotations for data columns, thereby improving the accuracy and efficiency of matching tasks.

Another significant development is the introduction of flow matching techniques, which offer a simulation-free approach to training continuous normalizing flows. These methods provide a more stable and robust alternative to traditional diffusion-based methods, particularly useful for handling large-scale schema matching tasks common in real-world applications. The ability to train these models at unprecedented scales allows for the efficient handling of complex data integration challenges.

The evaluation of generative models in schema matching also benefits from insights that emphasize the importance of evaluating these models based on their intended applications. This perspective is crucial in schema matching, where the ultimate goal is to ensure that the matched schemas are semantically consistent and can be effectively merged to support downstream data integration tasks.

In summary, the integration of generative models into schema matching evaluation offers a promising avenue for improving the accuracy, efficiency, and robustness of schema matching algorithms. By leveraging advanced techniques such as generative tags, flow matching, and context-aware evaluation metrics, researchers can develop more effective schema matching solutions that are better equipped to handle the complexities of real-world data integration challenges.

### 4.8 Task Scope and Context in LLM-Based Schema Matching

The application of Large Language Models (LLMs) to schema matching tasks is highly influenced by the scope and context provided. LLMs, such as those detailed in [155], [181], and [28], demonstrate potential in automating schema matching by leveraging their proficiency in understanding and generating natural language descriptions of schema elements. However, the task scope, defined by the amount and type of context information provided to the LLM, significantly impacts the quality of the matches generated.

In [4], researchers investigate various task scopes for prompting LLMs to perform schema matching, from minimal context to comprehensive descriptions. They observe that while richer context can enhance matching quality, it also complicates and lengthens the prompts, potentially overwhelming the LLM's processing capabilities. Balancing context richness with prompt manageability is thus crucial in designing LLM-based schema matching systems.

The decisiveness of the LLM's output, or its ability to confidently identify correct matches, is also influenced by the task scope. As noted in [4], newer LLMs tend to be more decisive, but this can be compromised by excessive or irrelevant context. Therefore, curating the context provided to the LLM is essential to ensure it enhances rather than hinders the matching process.

The integration of LLM-based schema matching with traditional string similarity methods is also emphasized in [4]. By combining the strengths of both approaches, data engineers can achieve a more robust and efficient schema matching process. This hybrid approach automates the initial matching phase, which can then be refined and verified using traditional methods.

In summary, the task scope and context provided to LLMs in schema matching tasks are critical factors determining the success of the matching process. By carefully designing prompts and selecting appropriate context information, LLMs can effectively bootstrap the schema matching process, reducing manual effort and enhancing the overall efficiency of data integration workflows.

### 4.9 Metric Elicitation for Schema Matching

In the context of schema matching, the selection of appropriate evaluation metrics is crucial for assessing the performance and effectiveness of matching algorithms. Metric elicitation for schema matching involves identifying and defining metrics that accurately reflect the quality of the matches generated by different methods. This process is essential for comparing various schema matching techniques and for guiding the development of new algorithms that can improve upon existing ones.

One of the key challenges in metric elicitation is the need to balance precision and recall. Precision measures the accuracy of the matches, while recall quantifies the proportion of true matches that are correctly identified. The F-score, which is the harmonic mean of precision and recall, provides a single metric that balances these two aspects [154]. However, in real-world applications, the relative importance of precision and recall may vary, necessitating the use of more nuanced metrics that can be tailored to specific use cases.

Metric elicitation can be approached as a problem of selecting the most appropriate performance metric for a given schema matching task. This involves understanding the implicit preferences and trade-offs of the users or stakeholders involved in the data integration process [182]. For instance, in some scenarios, users may prioritize the identification of all potential matches (high recall) over the need for perfect accuracy (high precision), while in other cases, the reverse may be true.

Recent advancements in machine learning and large language models (LLMs) have introduced new possibilities for metric elicitation in schema matching. For example, LLMs can be used to generate semantic correspondences between schema elements, which can then be evaluated using metrics that measure the quality of these matches [4]. Additionally, the use of generative models and embeddings can provide novel ways to assess the similarity between schema elements, leading to more robust and reliable matching results [29].

In summary, metric elicitation for schema matching is a multifaceted process that requires careful consideration of the specific requirements and constraints of the data integration task at hand. By leveraging advanced machine learning techniques and understanding user preferences, it is possible to develop more effective and efficient schema matching algorithms that can meet the needs of diverse applications. This process is critical for ensuring that the evaluation metrics align with the practical needs of schema integration tasks, thereby enhancing the overall performance and usability of the merged schemas.

### 4.10 Future Directions in Performance Metrics

Future directions in performance metrics for database schema matching and merging should focus on developing more sophisticated and context-sensitive evaluation criteria that can better capture the nuances of schema integration tasks. One promising avenue is the integration of multi-criteria decision analysis (MCDA) techniques, which can provide a more comprehensive assessment by considering multiple performance dimensions simultaneously [182]. This approach can help in balancing trade-offs between different metrics, such as precision and recall, to ensure that the merged schema not only aligns well with the source schemas but also maintains high usability and semantic coherence.

Another direction is the development of adaptive performance metrics that can dynamically adjust based on the specific characteristics of the schemas being matched and merged. This could involve the use of machine learning models to predict the most appropriate metrics for a given schema integration task, thereby improving the relevance and accuracy of the evaluation process [183]. Additionally, the incorporation of fairness and bias mitigation strategies in performance metrics is crucial, especially in scenarios where the schema matching and merging processes involve sensitive data or multiple stakeholder interests [185].

Furthermore, there is a need for performance metrics that can effectively evaluate the long-term sustainability and scalability of merged schemas. This includes assessing the robustness of the schema to changes in the underlying data sources and the ability to accommodate future schema evolution [186]. Finally, the development of user-centric performance metrics that incorporate feedback from domain experts and end-users can provide a more holistic evaluation of the schema integration outcomes, ensuring that the merged schema meets practical requirements and user expectations [184]. By exploring these future directions, performance metrics for database schema matching and merging can become more robust, adaptable, and aligned with the evolving needs of data integration tasks.

## 5 Challenges and Future Directions

### 5.1 Handling Schema Evolution

Handling schema evolution is a critical challenge in database schema matching and merging, particularly as databases and their schemas evolve over time to accommodate new requirements, data sources, and business needs. Schema evolution involves managing changes to the structure and organization of data, which can impact existing applications, queries, and data integrity. This subsection explores the complexities and potential solutions for managing schema evolution in various database systems.

One of the primary challenges in schema evolution is ensuring backward and forward compatibility. Backward compatibility ensures that new schemas can still process data stored in older formats, while forward compatibility ensures that older schemas can still process data stored in newer formats. This is particularly important in systems where data must be retained over long periods and accessed by multiple versions of applications.

NoSQL databases, which are often schema-less or have flexible schemas, present unique challenges. While they offer great flexibility in early stages of development, managing schema evolution becomes increasingly complex as the data becomes more heterogeneous. Solutions like the declarative schema evolution language provide a systematic approach to managing schema changes in NoSQL environments, allowing developers to adapt and change the implicit structure of the data stored.

For relational databases, online schema evolution is a significant challenge. Traditional approaches often require downtime or ad hoc patches, leading to incomplete functionality and reduced availability. Tesseract offers a novel approach by modeling schema evolution as data modification operations, enabling online, transactional schema evolution without service downtime.

Graph databases, such as those used in property graphs, also face schema evolution challenges. The lack of standardized data definition languages for property graphs complicates schema management. Approaches like PG-Schema, which features flexible type definitions and expressive constraints, aim to address these challenges by providing a robust framework for schema validation and evolution.

Ensuring query compatibility with evolving schemas is another critical aspect. Changes in schema can affect query results and data validity, making it essential to assess and accommodate these changes. Tools and frameworks that facilitate the analysis of schema relations and the reformulation of queries to maintain expected results across schema versions are crucial.

In summary, managing schema evolution requires a combination of systematic approaches, flexible schema management tools, and careful consideration of compatibility issues. By leveraging advancements in schema evolution languages, modeling schema changes as data modifications, and ensuring query compatibility, database systems can better adapt to the dynamic needs of modern applications.

### 5.2 Dealing with Large-Scale Data

Dealing with large-scale data in database schema matching and merging presents significant challenges that require innovative solutions. The sheer volume of data, often characterized by the "four Vs" (volume, velocity, variety, and veracity), necessitates the adoption of scalable and efficient methods to handle the complexities involved. Traditional approaches to schema matching and merging, which may work well for smaller datasets, often struggle with the computational and storage demands of big data environments.

One of the primary challenges is the need for scalable algorithms that can process large datasets without compromising accuracy. Techniques such as parallel processing and distributed computing, which leverage frameworks like Hadoop and Spark, have become essential for managing the computational load. These frameworks enable the decomposition of tasks into smaller, manageable units that can be processed concurrently across multiple nodes, thereby reducing the overall processing time.

Another critical aspect is the handling of data heterogeneity and uncertainty. Large-scale datasets often include a mix of structured, semi-structured, and unstructured data, each requiring different processing strategies. Probabilistic and approximate computation methods, which trade off some accuracy for improved scalability, are increasingly being employed to manage this heterogeneity. These methods allow for the efficient processing of large datasets while maintaining a reasonable level of accuracy.

Visualization techniques also play a crucial role in understanding and managing large-scale data. Tools that can effectively visualize high-dimensional and large-scale data help in identifying patterns and anomalies that might be missed through traditional analysis methods. Techniques like t-SNE and LargeVis have been developed to address the computational challenges of visualizing large datasets, making it easier to interpret complex data structures.

Finally, the integration of machine learning and statistical methods into schema matching and merging processes offers promising avenues for improving efficiency and accuracy. Machine learning models can be trained to recognize patterns and relationships within large datasets, enabling more intelligent and automated schema matching and merging processes.

In summary, dealing with large-scale data in database schema matching and merging requires a multi-faceted approach that combines scalable algorithms, probabilistic computation, advanced visualization techniques, and machine learning. These strategies collectively aim to address the unique challenges posed by big data, ensuring that schema matching and merging processes remain efficient and effective even as data volumes continue to grow.

### 5.3 Integrating Heterogeneous Data Sources

Integrating heterogeneous data sources presents significant challenges due to the diverse formats, structures, and semantics of the data involved. The heterogeneity can manifest in various forms, such as different data types (e.g., scalar, image, text), measurement scales (e.g., binary, ordinal, ratio), and even distinct computational paradigms (e.g., machine learning models, relational databases). Addressing these challenges requires innovative methods that can harmonize and consolidate data from disparate sources into a unified framework.

One approach to integrating heterogeneous data sources is through the use of tensor-based models. By representing both input data and output measurements as tensors, these models can capture complex relationships across different data types and dimensions. This tensor-based approach allows for the formulation of a linear regression model that can be optimized using techniques like alternating least squares (ALS), which provides a closed-form solution in each iteration, ensuring efficient computation.

Another strategy involves leveraging semantic technologies, such as ontologies, to manage and integrate heterogeneous data sources. An ontology-assisted system can enable users to query distributed heterogeneous data sources by abstracting away details like location, information structure, and access patterns, thereby facilitating interoperability and enhancing the semantic understanding of the integrated data.

In the context of statistical methods, techniques capable of fusing datasets with different types of heterogeneity, such as different omics data and measurements obtained at different scales, have been developed. These methods are evaluated through comprehensive simulations and real-world data analysis, demonstrating their effectiveness in handling the heterogeneity inherent in biological data.

Furthermore, architectural patterns like the mediator-wrapper architecture offer a modular and flexible solution for integrating heterogeneous data sources. The introduction of a mask component in this architecture allows for more efficient and scalable data integration, particularly in scenarios involving large-scale data processing.

Overall, the integration of heterogeneous data sources is a multifaceted problem that requires a combination of advanced statistical models, semantic technologies, and flexible architectural patterns. By adopting these approaches, researchers and practitioners can effectively consolidate and analyze data from diverse sources, leading to more robust and comprehensive insights.

### 5.4 Leveraging Machine Learning and AI Techniques

Leveraging Machine Learning (ML) and Artificial Intelligence (AI) techniques for database schema matching and merging offers significant potential to address the complexities and challenges inherent in these tasks. AI-driven approaches can automate the process of feature engineering, model selection, and hyperparameter tuning, thereby reducing manual effort and increasing the likelihood of finding optimal solutions. Techniques such as Bayesian optimization and reinforcement learning can dynamically adjust schema matching and merging algorithms based on the characteristics of the input data, which is crucial in scenarios where schemas may evolve over time or differ significantly across data sources.

AI techniques can also facilitate the development of more sophisticated and context-aware schema matching and merging systems. For instance, deep learning models can be trained to recognize patterns and relationships within schema structures, enabling more accurate and nuanced matching. Additionally, AI can predict and mitigate potential conflicts during the merging process, ensuring that the resulting schema is both coherent and functional.

However, the successful application of AI and ML in schema matching and merging requires addressing several challenges. These include robust data preprocessing, the development of effective evaluation metrics, and the integration of domain-specific knowledge into AI models. Ensuring the transparency, interpretability, and fairness of AI-driven schema matching and merging systems is essential to build trust and ensure their ethical use.

In conclusion, while the integration of AI and ML into database schema matching and merging is still evolving, the potential benefits are substantial. Future research should focus on developing more advanced AI techniques, improving model interpretability, and exploring the ethical implications of AI-driven schema management. By doing so, we can unlock the full potential of AI and ML to revolutionize the field of database schema management.

### 5.5 Utilizing Large Language Models (LLMs)

The integration of Large Language Models (LLMs) into database schema matching and merging presents both opportunities and challenges. LLMs, such as GPT-4, have demonstrated remarkable capabilities in natural language understanding and generation, which can be leveraged to enhance the accuracy and efficiency of schema matching tasks. However, the application of LLMs in this domain is not without its complexities.

One of the primary challenges is the domain specificity of LLMs. While LLMs are trained on vast corpora of text data, they may not inherently possess the specialized knowledge required for precise schema matching in niche database fields. This necessitates fine-tuning or domain-specific training to ensure that the LLM can accurately interpret and match schema elements.

Another significant issue is the potential for knowledge forgetting or repetition. LLMs may struggle to balance newly acquired information with previously learned data, leading to inconsistencies or repetitive outputs during schema matching. Techniques such as iterative methodologies and real-time learning feedback mechanisms can help mitigate these issues, ensuring that the LLM remains up-to-date and accurate.

The transparency and interpretability of LLMs also pose challenges. Schema matching often requires clear, interpretable outputs to ensure that the merged schema is coherent and functional. LLMs, particularly those that generate responses based on complex internal representations, may produce outputs that are difficult to trace back to their source data. Enhancing the transparency and interpretability of LLMs is crucial for their effective use in schema matching.

Moreover, the ethical considerations of using LLMs in database schema matching cannot be overlooked. LLMs may inadvertently generate biased or harmful outputs if their training data contains biases. Ensuring fairness and ethical standards in LLM outputs is essential to prevent unintended consequences in schema merging processes.

In conclusion, while LLMs offer promising avenues for improving database schema matching and merging, their effective utilization requires careful consideration of domain specificity, knowledge management, transparency, and ethical implications. Future research should focus on developing robust methodologies for fine-tuning LLMs to specific database domains, enhancing their transparency and interpretability, and ensuring that they adhere to high ethical standards.

### 5.6 Addressing Human-in-the-Loop Challenges

Addressing the challenges of incorporating human expertise into the loop of database schema matching and merging processes is a multifaceted endeavor. The integration of human-in-the-loop (HITL) approaches can significantly enhance the accuracy and relevance of schema matching and merging, particularly in complex and dynamic environments where automated systems may fall short. However, this integration presents several challenges that need to be carefully managed to ensure effective collaboration between human experts and machine learning algorithms.

One of the primary challenges is the misalignment of expectations, values, and constraints between strategic decision-makers (SDMs) and practical decision-makers (PDMs) involved in the process. SDMs often introduce HITL to optimize strategic and societal goals, while PDMs, who make the final decisions, may have different priorities and information needs. This misalignment can lead to conflicts and suboptimal outcomes, highlighting the need for clear communication and shared understanding between stakeholders.

Another challenge is the efficient and effective integration of human feedback into the machine learning pipeline. HITL systems must be designed to minimize the cognitive load on human experts while maximizing the value of their input. This requires sophisticated mechanisms for selecting the most informative instances for human review, as well as tools for providing meaningful and actionable feedback to the human experts.

Moreover, the scalability of HITL systems is a critical concern. As the volume of data and the complexity of schema matching and merging tasks increase, the ability to manage and process human feedback in real-time becomes increasingly challenging. Techniques such as active learning and adaptive sampling can help mitigate these issues by focusing human effort on the most critical aspects of the task.

Finally, the ethical and practical implications of HITL systems must be carefully considered. The role of human experts in the loop raises questions about accountability, transparency, and the potential for bias. Ensuring that HITL systems are designed with these considerations in mind is essential for their successful deployment in real-world applications.

In conclusion, while human-in-the-loop approaches offer significant potential for improving database schema matching and merging, they also present a range of challenges that must be addressed through careful design and implementation. By fostering clear communication, optimizing human-machine collaboration, and considering the ethical implications, we can harness the strengths of both human and machine intelligence to achieve more effective and reliable schema integration.

### 5.7 Future Research Directions

Future research directions in database schema matching and merging are poised to address several emerging challenges and leverage new technologies to enhance the accuracy, efficiency, and scalability of these processes. One promising direction is the integration of machine learning techniques to improve schema matching algorithms. By leveraging large datasets and advanced machine learning models, researchers can develop more robust and adaptive matching algorithms that can handle complex and heterogeneous schemas [212]. This approach could significantly reduce the manual effort required in schema matching and improve the overall accuracy of the matching process.

Another area of focus is the development of more sophisticated merging techniques that can handle conflicts and inconsistencies in schemas more effectively. This includes the use of semantic technologies and ontologies to provide a richer understanding of the data and to facilitate more intelligent merging decisions [217]. Additionally, the integration of natural language processing (NLP) techniques could enable the automatic extraction of schema information from unstructured data sources, further enhancing the merging process [211].

The advent of cloud computing and big data technologies presents new opportunities and challenges for schema matching and merging. Future research could explore the use of distributed computing frameworks to handle the large-scale data processing required for these tasks [213]. This includes the development of scalable algorithms that can operate efficiently in cloud environments and the exploration of new data storage and retrieval techniques that can support schema matching and merging at scale [214].

Moreover, the increasing importance of data privacy and security in data management systems necessitates the development of secure schema matching and merging techniques. Future research could focus on the integration of cryptographic methods and privacy-preserving techniques to ensure that sensitive data is protected during the schema matching and merging process [215].

Finally, the growing complexity of data ecosystems, including the proliferation of IoT devices and the increasing diversity of data sources, calls for the development of more flexible and adaptable schema matching and merging solutions. This includes the exploration of dynamic schema evolution techniques that can handle changes in data schemas over time and the development of interoperability frameworks that can facilitate the integration of data from diverse sources [216].

In summary, future research in database schema matching and merging will need to address the challenges posed by the increasing complexity and scale of data, the need for enhanced accuracy and efficiency, and the growing importance of data privacy and security. By leveraging advances in machine learning, semantic technologies, distributed computing, and data privacy techniques, researchers can develop more robust and adaptable solutions that meet the evolving needs of data management systems.

## 6 Conclusion

### 6.1 Summary of Key Findings

The survey on "The Methods of Database Schema Matching and Merging" provides a comprehensive overview of the current state-of-the-art techniques and methodologies in this domain. It highlights that schema matching and merging are critical processes in data integration, enabling the alignment and unification of disparate data sources. The survey emphasizes the importance of semantic understanding and context-aware techniques in improving the accuracy and efficiency of schema matching. Machine learning and natural language processing approaches have shown significant promise in automating and enhancing these processes, as they can capture complex patterns and relationships within the data schemas [219].

Furthermore, the survey underscores the necessity for robust evaluation frameworks to assess the performance of schema matching and merging algorithms. These frameworks should include diverse datasets and real-world scenarios to ensure the practical applicability of the methods [220]. The integration of user feedback and iterative refinement processes has been identified as a valuable strategy to enhance the reliability and user-acceptance of schema matching tools [219].

The survey also emphasizes the growing interest in developing scalable and efficient algorithms that can handle large-scale data integration tasks. This includes the exploration of distributed computing paradigms and the use of cloud-based solutions to manage the computational complexity and storage requirements associated with schema matching and merging [221].

In conclusion, the survey offers a holistic view of the current challenges and advancements in database schema matching and merging, providing valuable insights for researchers and practitioners. The findings suggest that ongoing research should focus on leveraging advanced computational techniques, enhancing evaluation methodologies, and addressing scalability issues to further advance the field [218].

### 6.2 Importance in Modern Data Management

In the contemporary landscape of data management, the importance of database schema matching and merging cannot be overstated. As data continues to proliferate in both volume and variety, the ability to seamlessly integrate and harmonize disparate data sources becomes increasingly critical [222]. This capability is essential for ensuring that data can be efficiently utilized across various applications, from business intelligence to scientific research [226]. The integration of schema matching and merging techniques allows organizations to break down data silos, enabling a more holistic view of information that can drive informed decision-making [223].

Modern data management systems are not just about storing and retrieving data; they are also about ensuring data quality, consistency, and accessibility [224]. The shift towards data-centric architectures, as seen in systems like CondorJ2, underscores the need for flexible and scalable data management solutions that can adapt to the dynamic nature of data [224]. Moreover, the advent of large language models (LLMs) and machine learning techniques has further elevated the importance of robust data management practices, as these technologies rely heavily on the quality and structure of the underlying data [225].

The challenges of data management in complex, multi-disciplinary projects, such as those in the BIG-MAP project, highlight the necessity for comprehensive data management plans that address issues of data interoperability and FAIR principles [223]. These plans are crucial for ensuring that data can be effectively shared, reused, and understood by diverse stakeholders, thereby maximizing the value of the data [223].

In summary, the role of database schema matching and merging in modern data management is pivotal. It facilitates the integration of diverse data sources, ensuring that data remains accessible, consistent, and valuable across various domains and applications. As data continues to evolve and expand, the ability to manage and harmonize schemas will remain a cornerstone of effective data management practices [222].

### 6.3 Real-World Applications and Impact

The application of database schema matching and merging techniques has significant real-world implications across various domains, enhancing data integration, interoperability, and overall system efficiency. In low-resource regions, such as those in the Global South, these techniques can facilitate the deployment of AI-driven tools that address local needs [230]. By ensuring that disparate data sources can be harmonized and merged, these technologies enable more effective data-driven decision-making, which is crucial in environments where data fragmentation and lack of standardization are common challenges.

In the realm of robotics, accurate schema matching and merging are essential for validating simulation environments against real-world data [228]. This is particularly important in dynamic scenarios involving high-speed impact events, where the ability to merge sensor data from simulations with real-world observations can significantly improve the reliability and accuracy of robotic systems. Such advancements are pivotal for applications in autonomous driving, where real-time data integration is critical for safety and performance.

The integration of these techniques into 6G networks promises to revolutionize data management and processing, particularly in the context of massive data traffic and intelligent services [229]. By enabling seamless merging of data from diverse sources, 6G networks can support advanced applications such as virtual reality, smart healthcare, and Industry 5.0 [227]. This integration is also crucial for addressing privacy and security concerns, as it allows for more granular and effective data management strategies [229].

In the field of AI, schema matching and merging play a vital role in ensuring the fairness and accuracy of machine learning models, especially in sensitive applications like medical imaging [231]. By merging data from different schemas, researchers can create more comprehensive datasets that reduce bias and improve model performance. This is particularly important in model selection, where the choice of metrics can significantly impact the outcomes [231].

Overall, the real-world applications of database schema matching and merging are vast and varied, spanning from AI deployment in low-resource regions to the validation of robotics simulations and the development of next-generation wireless networks. These techniques not only enhance data integration and interoperability but also pave the way for more intelligent, efficient, and secure systems across multiple domains.

### 6.4 Future Research Directions


As the field of database schema matching and merging continues to evolve, several promising future research directions emerge that could significantly enhance the efficiency, accuracy, and scalability of these processes. One key area of focus is the integration of advanced machine learning techniques to improve the automatic detection and resolution of schema discrepancies. By leveraging the vast amounts of data available, machine learning models can be trained to recognize patterns and relationships that are not easily discernible through traditional rule-based methods [234]. This approach could lead to more robust and adaptive schema matching algorithms that can handle the increasing complexity of modern databases.

Another promising direction is the development of more sophisticated semantic matching techniques. Current methods often rely on syntactic similarities, which can be insufficient for capturing the nuanced relationships between schema elements. Incorporating semantic information, such as ontologies and domain-specific knowledge, could enhance the accuracy of matching and merging processes [232]. This would require advancements in natural language processing and knowledge representation, but the potential benefits could be substantial.

The integration of blockchain technology into schema management is another intriguing prospect. Blockchain's inherent properties of immutability and transparency could provide a secure and verifiable framework for tracking schema changes and ensuring data integrity [213]. This could be particularly valuable in environments where data governance and compliance are critical concerns.

Furthermore, the advent of 6G networks and the increasing prevalence of edge computing present new opportunities and challenges for database schema management. The need for real-time data processing and the decentralization of data storage and computation will require innovative approaches to schema matching and merging that can operate efficiently in distributed and dynamic environments [235].

Finally, the growing importance of privacy and security in data management necessitates research into privacy-preserving schema matching and merging techniques. Ensuring that sensitive information is protected while still enabling effective schema integration is a complex but essential area of study [233].

In conclusion, the future of database schema matching and merging is likely to be shaped by advancements in machine learning, semantic technologies, blockchain, and privacy-preserving methods, as well as by the evolving landscape of network and computing architectures. Addressing these challenges will require interdisciplinary collaboration and a commitment to continuous innovation.


### 6.5 Conclusion and Final Thoughts

The study of database schema matching and merging has evolved significantly, driven by the need for efficient data integration and interoperability across diverse systems. This survey has explored various methodologies and techniques that have been developed to address the challenges inherent in these processes. The methods discussed range from rule-based and statistical approaches to more advanced machine learning and semantic techniques, each offering unique advantages and trade-offs in terms of accuracy, scalability, and computational complexity.

One of the key takeaways from this survey is the importance of context-aware matching, where the context in which data is used plays a crucial role in determining the most appropriate matching strategy. This is particularly evident in the use of domain-specific ontologies and semantic web technologies, which have shown promise in improving the precision of schema matching. Additionally, the integration of machine learning algorithms, such as neural networks and decision trees, has demonstrated the potential to automate and enhance the matching process, reducing the reliance on manual intervention.

The merging aspect of schema integration has also seen advancements, with techniques like schema alignment and transformation becoming more sophisticated. These methods aim to reconcile differences in schema structures while preserving data integrity and consistency. The use of graph-based representations and optimization algorithms has been particularly effective in this regard, enabling the efficient merging of complex schemas.

Looking ahead, the field of database schema matching and merging is poised for continued innovation. The increasing availability of big data and the growing complexity of data ecosystems will likely drive the development of more robust and scalable solutions. Furthermore, the integration of artificial intelligence and natural language processing techniques could lead to more intuitive and user-friendly tools for schema management. As new technologies and techniques emerge, it is essential for researchers and practitioners to remain adaptable and open to innovation, ensuring that data integration remains a cornerstone of effective data management and analysis.


## References

[1] Schema Integration on Massive Data Sources. https://arxiv.org/abs/1808.01620

[2] The Role of Schema Matching in Large Enterprises. https://arxiv.org/abs/0909.1771

[3] A Semi-Automated Hybrid Schema Matching Framework for Vegetation Data  Integration. https://arxiv.org/abs/2305.06528

[4] Schema Matching with Large Language Models: an Experimental Study. https://arxiv.org/abs/2407.11852

[5] Survive the Schema Changes: Integration of Unmanaged Data Using Deep  Learning. https://arxiv.org/abs/2010.07586

[6] A Theory of Concepts and Their Combinations I: The Structure of the Sets  of Contexts and Properties. https://arxiv.org/abs/quant-ph/0402207

[7] Protoperads i: Combinatorics and definitions. https://arxiv.org/abs/1901.05653

[8] Kinds of concepts. https://arxiv.org/abs/1112.6124

[9] The concepts of work and heat and the first and second laws of  thermodynamics. https://arxiv.org/abs/1203.2294

[10] Developer Experience: Concept and Definition. https://arxiv.org/abs/1312.1452

[11] A General Theory of Concept Lattice (I): Emergence of General Concept  Lattice. https://arxiv.org/abs/1908.01056

[12] Fibrational linguistics: First concepts. https://arxiv.org/abs/2201.01136

[13] Clustering -- Basic concepts and methods. https://arxiv.org/abs/2212.01248

[14] Categories of Semantic Concepts. https://arxiv.org/abs/2004.10741

[15] A General Theory of Concept Lattice (II): Tractable Lattice Construction  and Implication Extraction. https://arxiv.org/abs/1908.04260

[16] Set theory and topology. An introduction to the foundations of analysis.  Part II: Topology - Fundamental notions. https://arxiv.org/abs/1306.6926

[17] Fundamental Concepts. https://arxiv.org/abs/1810.07032

[18] Live & Local Schema Change: Challenge Problems. https://arxiv.org/abs/2309.11406

[19] WinoGrande: An Adversarial Winograd Schema Challenge at Scale. https://arxiv.org/abs/1907.10641

[20] SMUTF: Schema Matching Using Generative Tags and Hybrid Features. https://arxiv.org/abs/2402.01685

[21] Technical Report: Optimizing Human Involvement for Entity Matching and  Consolidation. https://arxiv.org/abs/1906.06574

[22] BSML: A Binding Schema Markup Language for Data Interchange in Problem  Solving Environments (PSEs). https://arxiv.org/abs/cs/0202027

[23] Composition and Inversion of Schema Mappings. https://arxiv.org/abs/0910.3372

[24] Reducing Uncertainty of Schema Matching via Crowdsourcing with Accuracy  Rates. https://arxiv.org/abs/1809.04017

[25] Improving Schema Matching with Linked Data. https://arxiv.org/abs/1205.2691

[26] Merging 1D and 3D genomic information: Challenges in modelling and  validation. https://arxiv.org/abs/2005.06755

[27] Background Knowledge in Schema Matching: Strategy vs. Data. https://arxiv.org/abs/2107.00001

[28] Matchmaker: Self-Improving Large Language Model Programs for Schema  Matching. https://arxiv.org/abs/2410.24105

[29] It's AI Match: A Two-Step Approach for Schema Matching Using Embeddings. https://arxiv.org/abs/2203.04366

[30] General Context-Aware Data Matching and Merging Framework. https://arxiv.org/abs/1807.10009

[31] Cost-Aware Uncertainty Reduction in Schema Matching with GPT-4: The  Prompt-Matcher Framework. https://arxiv.org/abs/2408.14507

[32] An Automatic Schema-Instance Approach for Merging Multidimensional Data  Warehouses. https://arxiv.org/abs/2107.12055

[33] Set-merging for the Matching Algorithm of Micali and Vazirani. https://arxiv.org/abs/1501.00212

[34] Formulating A Strategic Plan Based On Statistical Analyses And  Applications For Financial Companies Through A Real-World Use Case. https://arxiv.org/abs/2307.04778

[35] Automatic Generation of Acceptance Test Cases from Use Case  Specifications: an NLP-based Approach. https://arxiv.org/abs/1907.08490

[36] Practical Adoption of Cloud Computing in Power Systems- Drivers,  Challenges, Guidance, and Real-world Use Cases. https://arxiv.org/abs/2108.00303

[37] Tagging Real-World Scenarios for the Assessment of Autonomous Vehicles. https://arxiv.org/abs/2012.01081

[38] Use Scenarios & Practical Examples of AI Use in Education. https://arxiv.org/abs/2309.12320

[39] 5G as Enabler for Industrie 4.0 Use Cases: Challenges and Concepts. https://arxiv.org/abs/2410.08726

[40] Use-cases of Blockchain Technology for Humanitarian Engineering. https://arxiv.org/abs/2012.01168

[41] Applications of artificial intelligence in drug development using  real-world data. https://arxiv.org/abs/2101.08904

[42] Open Case Studies: Statistics and Data Science Education through  Real-World Applications. https://arxiv.org/abs/2301.05298

[43] REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous  Manipulation. https://arxiv.org/abs/2309.03322

[44] Mathematical Models in Schema Theory. https://arxiv.org/abs/cs/0512099

[45] An Overview of Schema Theory. https://arxiv.org/abs/1401.2651

[46] A Human-Machine Collaboration Framework for the Development of Schemas. https://arxiv.org/abs/2402.07932

[47] Towards Schema Inference for Data Lakes. https://arxiv.org/abs/2206.03881

[48] Causal schema induction for knowledge discovery. https://arxiv.org/abs/2303.15381

[49] The Exact Schema Theorem. https://arxiv.org/abs/1105.3538

[50] Dynamic Integration of Background Knowledge in Neural NLU Systems. https://arxiv.org/abs/1706.02596

[51] Incorporating Semantic Knowledge into Latent Matching Model in Search. https://arxiv.org/abs/1604.06270

[52] Preliminary Guidelines For Combining Data Integration and Visual Data  Analysis. https://arxiv.org/abs/2403.04757

[53] Combining Visual Analytics and Content Based Data Retrieval Technology  for Efficient Data Analysis. https://arxiv.org/abs/1506.07915

[54] Visual Analysis of Ontology Matching Results with the MELT Dashboard. https://arxiv.org/abs/2004.12628

[55] Euler/X: A Toolkit for Logic-based Taxonomy Integration. https://arxiv.org/abs/1402.1992

[56] A logic-based approach to data integration. https://arxiv.org/abs/cs/0110032

[57] Information Integration and Computational Logic. https://arxiv.org/abs/cs/0106025

[58] Stratified Data Integration. https://arxiv.org/abs/2105.09432

[59] Constraint Programming viewed as Rule-based Programming. https://arxiv.org/abs/cs/0003076

[60] A Generalized Method for Integrating Rule-based Knowledge into Inductive  Methods Through Virtual Sample Creation. https://arxiv.org/abs/1101.4924

[61] RuleVis: Constructing Patterns and Rules for Rule-Based Models. https://arxiv.org/abs/1911.04638

[62] RRULES: An improvement of the RULES rule-based classifier. https://arxiv.org/abs/2106.07296

[63] Techniques for Automated Machine Learning. https://arxiv.org/abs/1907.08908

[64] Automated Machine Learning: From Principles to Practices. https://arxiv.org/abs/1810.13306

[65] ACL2(ml): Machine-Learning for ACL2. https://arxiv.org/abs/1404.3034

[66] Machine Learned Learning Machines. https://arxiv.org/abs/1705.10201

[67] Techniques for Interpretable Machine Learning. https://arxiv.org/abs/1808.00033

[68] Bayesian Probabilistic Numerical Methods. https://arxiv.org/abs/1702.03673

[69] Three Approaches to Probability Model Selection. https://arxiv.org/abs/1302.6838

[70] Integrating heterogeneous knowledges for understanding biological  behaviors: a probabilistic approach. https://arxiv.org/abs/0712.3900

[71] Structural and semantic pattern matching analysis in Haskell. https://arxiv.org/abs/1909.04160

[72] Extracting Syntactic Patterns from Databases. https://arxiv.org/abs/1710.11528

[73] Syntactic Patterns Improve Information Extraction for Medical Search. https://arxiv.org/abs/1805.00097

[74] Classifying Syntactic Regularities for Hundreds of Languages. https://arxiv.org/abs/1603.08016

[75] An Inter-lingual Reference Approach For Multi-Lingual Ontology Matching. https://arxiv.org/abs/1309.6650

[76] A Syntactic Approach to Studying Strongly Equivalent Logic Programs. https://arxiv.org/abs/2011.04454

[77] Syntax-Semantics Interaction Parsing Strategies. Inside SYNTAGMA. https://arxiv.org/abs/1601.05768

[78] SYNTAGMA. A Linguistic Approach to Parsing. https://arxiv.org/abs/1303.5960

[79] A Fast Generic Sequence Matching Algorithm. https://arxiv.org/abs/0810.0264

[80] Question Answering System Using Syntactic Information. https://arxiv.org/abs/cs/9911006

[81] Semantic Matching Against a Corpus: New Applications and Methods. https://arxiv.org/abs/1808.09502

[82] Finding Salient Context based on Semantic Matching for Relevance Ranking. https://arxiv.org/abs/1909.01165

[83] Semi-Supervised Semantic Matching. https://arxiv.org/abs/1901.08339

[84] A New Approach for Semantic Web Matching. https://arxiv.org/abs/2107.06083

[85] A hybrid model approach for strange and multi-strange hadrons in 2.76 A  TeV Pb+Pb collisions. https://arxiv.org/abs/1501.03286

[86] What are Hybrid Development Methods Made Of? An Evidence-based  Characterization. https://arxiv.org/abs/2101.08016

[87] Inference in Hybrid Networks: Theoretical Limits and Practical  Algorithms. https://arxiv.org/abs/1301.2288

[88] Towards the statistical construction of hybrid development methods. https://arxiv.org/abs/2105.13563

[89] How to Work a Crowd: Developing Crowd Capital Through Crowdsourcing. https://arxiv.org/abs/1702.04214

[90] An Analytic Approach to People Evaluation in Crowdsourcing Systems. https://arxiv.org/abs/1211.3200

[91] Towards AI-Empowered Crowdsourcing. https://arxiv.org/abs/2212.14676

[92] Clustering Via Crowdsourcing. https://arxiv.org/abs/1604.01839

[93] A Perspective on Crowdsourcing and Human-in-the-Loop Workflows in  Precision Health. https://arxiv.org/abs/2303.03578

[94] Graphs as Tools to Improve Deep Learning Methods. https://arxiv.org/abs/2110.03999

[95] Explaining Deep Neural Networks and Beyond: A Review of Methods and  Applications. https://arxiv.org/abs/2003.07631

[96] Clustering with Deep Learning: Taxonomy and New Methods. https://arxiv.org/abs/1801.07648

[97] Machine learning and deep learning. https://arxiv.org/abs/2104.05314

[98] Methods for Interpreting and Understanding Deep Neural Networks. https://arxiv.org/abs/1706.07979

[99] LLMs4OM: Matching Ontologies with Large Language Models. https://arxiv.org/abs/2404.10317

[100] Fine-tuning Large Language Models for Entity Matching. https://arxiv.org/abs/2409.08185

[101] Entity Matching using Large Language Models. https://arxiv.org/abs/2310.11244

[102] Self-Improving Algorithms. https://arxiv.org/abs/0907.0884

[103] Self-Adaptive Training: Bridging Supervised and Self-Supervised Learning. https://arxiv.org/abs/2101.08732

[104] Self-Adaptive Systems in Organic Computing: Strategies for  Self-Improvement. https://arxiv.org/abs/1808.03519

[105] Applying Machine Learning in Self-Adaptive Systems: A Systematic  Literature Review. https://arxiv.org/abs/2103.04112

[106] A Generalization of Self-Improving Algorithms. https://arxiv.org/abs/2003.08329

[107] Data Source Selection for Information Integration in Big Data Era. https://arxiv.org/abs/1610.09506

[108] Heterogeneous large datasets integration using Bayesian factor  regression. https://arxiv.org/abs/1810.09894

[109] LDQL: A Query Language for the Web of Linked Data (Extended Version). https://arxiv.org/abs/1507.04614

[110] Semantic Technology to Exploit Digital Content Exposed as Linked Data. https://arxiv.org/abs/1110.2396

[111] Integrating Know-How into the Linked Data Cloud. https://arxiv.org/abs/1604.04506

[112] Biological data integration using Semantic Web technologies. https://arxiv.org/abs/0902.3147

[113] Supporting interoperability of collaborative networks through  engineering of a service-based Mediation Information System (MISE 2.0). https://arxiv.org/abs/1509.09152

[114] A multi-protocol framework for the development of collaborative virtual  environments. https://arxiv.org/abs/1412.3260

[115] Three-dimensional conceptual model for service-oriented simulation. https://arxiv.org/abs/0909.3414

[116] A Survey on Service Composition Middleware in Pervasive Environments. https://arxiv.org/abs/0909.2183

[117] Knowledge Integration of Collaborative Product Design Using Cloud  Computing Infrastructure. https://arxiv.org/abs/2001.09796

[118] CoRL: Environment Creation and Management Focused on System Integration. https://arxiv.org/abs/2303.02182

[119] Interface Matching and Combining Techniques for Services Integration. https://arxiv.org/abs/0807.3933

[120] Integrating Information Technology in Healthcare: Recent Developments,  Challenges, and Future Prospects for Urban and Regional Health. https://arxiv.org/abs/2307.16296

[121] Bioinformatics and Medicine in the Era of Deep Learning. https://arxiv.org/abs/1802.09791

[122] Federated Learning for Healthcare Informatics. https://arxiv.org/abs/1911.06270

[123] Evaluation of GPT-3.5 and GPT-4 for supporting real-world information  needs in healthcare delivery. https://arxiv.org/abs/2304.13714

[124] Introduction to Bioinformatics. https://arxiv.org/abs/0911.4230

[125] Data Resources for Structural Bioinformatics. https://arxiv.org/abs/2307.02171

[126] e-commerce business models in the context of web3.0 paradigm. https://arxiv.org/abs/1401.6102

[127] Architecture of Smart Certificates for Web3 Applications Against  Cyberthreats in Financial Industry. https://arxiv.org/abs/2311.01956

[128] When Digital Economy Meets Web3.0: Applications and Challenges. https://arxiv.org/abs/2210.08993

[129] Classifications as Linked Open Data. Challenges and Opportunities. https://arxiv.org/abs/2204.14066

[130] Preliminary results on Ontology-based Open Data Publishing. https://arxiv.org/abs/1705.10480

[131] Towards Integrated and Open COVID-19 Data. https://arxiv.org/abs/2008.04045

[132] Ethics of Open Data. https://arxiv.org/abs/2205.10402

[133] Combining Public and Private Data. https://arxiv.org/abs/2111.00115

[134] Open Data Quality. https://arxiv.org/abs/2007.06540

[135] Talking Open Data. https://arxiv.org/abs/1705.00894

[136] Blended Integrated Open Data: dados abertos p\'ublicos integrados. https://arxiv.org/abs/1909.00743

[137] Understanding and Preparing Data of Industrial Processes for Machine  Learning Applications. https://arxiv.org/abs/2109.03469

[138] AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data  Readiness for AI. https://arxiv.org/abs/2406.19256

[139] tf.data: A Machine Learning Data Processing Framework. https://arxiv.org/abs/2101.12127

[140] Integration of knowledge and data in machine learning. https://arxiv.org/abs/2202.10337

[141] Data Representativity for Machine Learning and AI Systems. https://arxiv.org/abs/2203.04706

[142] Modeling, Analysis, and Hard Real-time Scheduling of Adaptive Streaming  Applications. https://arxiv.org/abs/1807.04835

[143] Integration of streaming services and TCP data transmission in the  Internet. https://arxiv.org/abs/cs/0601016

[144] A Survey of Approaches and Challenges in the Real-time Multimedia  Streaming. https://arxiv.org/abs/1610.03039

[145] Approximate Integration of streaming data. https://arxiv.org/abs/1709.04290

[146] Object Oriented Approach for Integration of Heterogeneous Databases in a  Multidatabase System and Local Schemas Modifications Propagation. https://arxiv.org/abs/0912.0603

[147] Enhanced Inversion of Schema Evolution with Provenance. https://arxiv.org/abs/2211.13810

[148] Toward Safe Integration of Legacy SCADA Systems in the Smart Grid. https://arxiv.org/abs/2107.05863

[149] Living in Parallel Realities -- Co-Existing Schema Versions with a  Bidirectional Database Evolution Language. https://arxiv.org/abs/1608.05564

[150] Accelerating Human-in-the-loop Machine Learning: Challenges and  Opportunities. https://arxiv.org/abs/1804.05892

[151] Human-in-the-Loop Schema Induction. https://arxiv.org/abs/2302.13048

[152] PoWareMatch: a Quality-aware Deep Learning Approach to Improve Human  Schema Matching. https://arxiv.org/abs/2109.07321

[153] Human Schema Curation via Causal Association Rule Mining. https://arxiv.org/abs/2104.08811

[154] Schema Matching using Machine Learning. https://arxiv.org/abs/1911.11543

[155] ReMatch: Retrieval Enhanced Schema Matching with LLMs. https://arxiv.org/abs/2403.01567

[156] Optimizing F-measure: A Tale of Two Approaches. https://arxiv.org/abs/1206.4625

[157] F*: An Interpretable Transformation of the F-measure. https://arxiv.org/abs/2008.00103

[158] Statistical Formulas for F Measures. https://arxiv.org/abs/2012.14894

[159] Computing Multi-Relational Sufficient Statistics for Large Databases. https://arxiv.org/abs/1408.5389

[160] On the Scalability of Multidimensional Databases. https://arxiv.org/abs/1103.3753

[161] Scalable Blocking for Very Large Databases. https://arxiv.org/abs/2008.08285

[162] A Study of the Fundamental Performance Characteristics of GPUs and CPUs  for Database Analytics (Extended Version). https://arxiv.org/abs/2003.01178

[163] A comparison of the performance and scalability of relational and  document-based web-systems for large scale applications in a rehabilitation  context. https://arxiv.org/abs/1510.00216

[164] The Impact of Distance on Performance and Scalability of Distributed  Database Systems in Hybrid Clouds. https://arxiv.org/abs/2007.15826

[165] Optimizing Non-decomposable Performance Measures: A Tale of Two Classes. https://arxiv.org/abs/1505.06812

[166] A Unified View of Multi-Label Performance Measures. https://arxiv.org/abs/1609.00288

[167] Selecting a classification performance measure: matching the measure to  the problem. https://arxiv.org/abs/2409.12391

[168] Evaluation of Performance Measures for Classifiers Comparison. https://arxiv.org/abs/1112.4133

[169] A Survey of Human-in-the-loop for Machine Learning. https://arxiv.org/abs/2108.00941

[170] Investigating Positive and Negative Qualities of Human-in-the-Loop  Optimization for Designing Interaction Techniques. https://arxiv.org/abs/2204.07641

[171] Dynamic Human Evaluation for Relative Model Comparisons. https://arxiv.org/abs/2112.08048

[172] On Search Engine Evaluation Metrics. https://arxiv.org/abs/1302.2318

[173] Data Separability for Neural Network Classifiers and the Development of  a Separability Index. https://arxiv.org/abs/2005.13120

[174] A Novel Intrinsic Measure of Data Separability. https://arxiv.org/abs/2109.05180

[175] Separability is not the best goal for machine learning. https://arxiv.org/abs/1807.02873

[176] Learning with partially separable data. https://arxiv.org/abs/2103.06869

[177] Some aspects of separability. https://arxiv.org/abs/quant-ph/0112177

[178] A classification performance evaluation measure considering data  separability. https://arxiv.org/abs/2211.05433

[179] A note on the evaluation of generative models. https://arxiv.org/abs/1511.01844

[180] Flow Matching for Generative Modeling. https://arxiv.org/abs/2210.02747

[181] KcMF: A Knowledge-compliant Framework for Schema and Entity Matching  with Fine-tuning-free LLMs. https://arxiv.org/abs/2410.12480

[182] Classification Performance Metric Elicitation and its Applications. https://arxiv.org/abs/2208.09142

[183] Model-based metrics: Sample-efficient estimates of predictive model  subpopulation performance. https://arxiv.org/abs/2104.12231

[184] A Survey and Implementation of Performance Metrics for Self-Organized  Maps. https://arxiv.org/abs/2011.05847

[185] Fair Performance Metric Elicitation. https://arxiv.org/abs/2006.12732

[186] Deeper and Wider Networks for Performance Metrics Prediction in  Communication Networks. https://arxiv.org/abs/2401.00429

[187] Ensuring Query Compatibility with Evolving XML Schemas. https://arxiv.org/abs/0811.4324

[188] Managing Schema Evolution in NoSQL Data Stores. https://arxiv.org/abs/1308.0514

[189] Schema Validation and Evolution for Graph Databases. https://arxiv.org/abs/1902.06427

[190] PG-Schema: Schemas for Property Graphs. https://arxiv.org/abs/2211.10962

[191] Online Schema Evolution is (Almost) Free for Snapshot Databases. https://arxiv.org/abs/2210.03958

[192] Analysis of Distributed Algorithms for Big-data. https://arxiv.org/abs/2404.06461

[193] Big Data: Understanding Big Data. https://arxiv.org/abs/1601.04602

[194] Visualizing Large-scale and High-dimensional Data. https://arxiv.org/abs/1602.00370

[195] Approximate Computation and Implicit Regularization for Very Large-scale  Data Analysis. https://arxiv.org/abs/1203.0786

[196] Big Data. https://arxiv.org/abs/2012.09109

[197] Dealing with Big Data. https://arxiv.org/abs/1605.06354

[198] Mask-Mediator-Wrapper: A revised mediator-wrapper architecture for  heterogeneous data source integration. https://arxiv.org/abs/2208.12319

[199] Fusing heterogeneous data sets. https://arxiv.org/abs/1908.09653

[200] A novel approach for fusion of heterogeneous sources of data. https://arxiv.org/abs/1803.00138

[201] Semantic Information Retrieval from Distributed Heterogeneous Data  Sources. https://arxiv.org/abs/0707.0745

[202] Fostering Trust and Quantifying Value of AI and ML. https://arxiv.org/abs/2407.05919

[203] Deep Learning and Machine Learning, Advancing Big Data Analytics and  Management: Unveiling AI's Potential Through Tools, Techniques, and  Applications. https://arxiv.org/abs/2410.01268

[204] Uncovering Key Trends in Industry 5.0 through Advanced AI Techniques. https://arxiv.org/abs/2410.16748

[205] Artificial Intelligence and Machine Learning in 5G Network Security:  Opportunities, advantages, and future research trends. https://arxiv.org/abs/2007.04490

[206] Challenges and Contributing Factors in the Utilization of Large Language  Models (LLMs). https://arxiv.org/abs/2310.13343

[207] Challenging the Human-in-the-loop in Algorithmic Decision-making. https://arxiv.org/abs/2405.10706

[208] Human-in-the-Loop Methods for Data-Driven and Reinforcement Learning  Systems. https://arxiv.org/abs/2008.13221

[209] Dialogue Learning With Human-In-The-Loop. https://arxiv.org/abs/1611.09823

[210] Improving the Efficiency of Human-in-the-Loop Systems: Adding Artificial  to Human Experts. https://arxiv.org/abs/2307.03003

[211] Current Trends and Future Research Directions for Interactive Music. https://arxiv.org/abs/1810.04276

[212] Data-driven Innovation: Understanding the Direction for Future Research. https://arxiv.org/abs/2212.03061

[213] A Manifesto for Future Generation Cloud Computing: Research Directions  for the Next Decade. https://arxiv.org/abs/1711.09123

[214] New technological trajectories and research directions in Cloud  Computing Technology, 2004-2021. https://arxiv.org/abs/2207.12093

[215] Questions to Guide the Future of Artificial Intelligence Research. https://arxiv.org/abs/1912.10305

[216] Five Disruptive Technology Directions for 5G. https://arxiv.org/abs/1312.0229

[217] Next Generation Cloud Computing: New Trends and Research Directions. https://arxiv.org/abs/1707.07452

[218] The Road Towards 6G: A Comprehensive Survey. https://arxiv.org/abs/2102.01420

[219] Summary Explorer: Visualizing the State of the Art in Text Summarization. https://arxiv.org/abs/2108.01879

[220] Gaia Data Release 1. Summary of the astrometric, photometric, and survey  properties. https://arxiv.org/abs/1609.04172

[221] An Abstracted Survey on 6G: Drivers, Requirements, Efforts, and Enablers. https://arxiv.org/abs/2101.01062

[222] Data Management: Past, Present, and Future. https://arxiv.org/abs/cs/0701156

[223] Data Management Plans: the Importance of Data Management in the BIG-MAP  Project. https://arxiv.org/abs/2106.01616

[224] Turning Cluster Management into Data Management: A System Overview. https://arxiv.org/abs/cs/0612137

[225] LLM-Enhanced Data Management. https://arxiv.org/abs/2402.02643

[226] Scientific Data Management in the Coming Decade. https://arxiv.org/abs/cs/0502008

[227] A Comprehensive Survey on 6G Networks:Applications, Core Services,  Enabling Technologies, and Future Challenges. https://arxiv.org/abs/2101.12475

[228] Validating Robotics Simulators on Real-World Impacts. https://arxiv.org/abs/2110.00541

[229] 6G Survey on Challenges, Requirements, Applications, Key Enabling  Technologies, Use Cases, AI integration issues and Security aspects. https://arxiv.org/abs/2206.00868

[230] AI in the "Real World": Examining the Impact of AI Deployment in  Low-Resource Contexts. https://arxiv.org/abs/2012.01165

[231] Model Selection's Disparate Impact in Real-World Deep Learning  Applications. https://arxiv.org/abs/2104.00606

[232] Multimodal Classification: Current Landscape, Taxonomy and Future  Directions. https://arxiv.org/abs/2109.09020

[233] A Survey on Privacy for B5G/6G: New Privacy Challenges, and Research  Directions. https://arxiv.org/abs/2203.04264

[234] Identification of promising research directions using machine learning  aided medical literature analysis. https://arxiv.org/abs/1607.04660

[235] Wireless Federated Learning (WFL) for 6G Networks -- Part I: Research  Challenges and Future Trends. https://arxiv.org/abs/2105.00842


