{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "import sqlite3\n",
    "import re\n",
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange, tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import requests\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Local imports (if any)\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to DEBUG for more detailed output\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)  # Logs will be output to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Prompts\n",
    "\n",
    "# Outline Generation Prompts\n",
    "ROUGH_OUTLINE_PROMPT = '''\n",
    "You want to write an overall and comprehensive academic survey about \"[TOPIC]\".\n",
    "You are provided with a list of papers related to the topic below:\n",
    "---\n",
    "[PAPER LIST]\n",
    "---\n",
    "You need to draft an outline based on the given papers.\n",
    "The outline should contain a title and several sections.\n",
    "Each section follows with a brief sentence to describe what to write in this section.\n",
    "The outline is supposed to be comprehensive and contains [SECTION NUM] sections.\n",
    "\n",
    "Return in the format:\n",
    "<format>\n",
    "Title: [TITLE OF THE SURVEY]\n",
    "Section 1: [NAME OF SECTION 1]\n",
    "Description 1: [DESCRIPTION OF SECTION 1]\n",
    "\n",
    "Section 2: [NAME OF SECTION 2]\n",
    "Description 2: [DESCRIPTION OF SECTION 2]\n",
    "\n",
    "...\n",
    "\n",
    "Section K: [NAME OF SECTION K]\n",
    "Description K: [DESCRIPTION OF SECTION K]\n",
    "</format>\n",
    "The outline:\n",
    "'''\n",
    "\n",
    "MERGING_OUTLINE_PROMPT = '''\n",
    "You are an expert in artificial intelligence who wants to write an overall survey about [TOPIC].\n",
    "You are provided with a list of outlines as candidates below:\n",
    "---\n",
    "[OUTLINE LIST]\n",
    "---\n",
    "Each outline contains a title and several sections.\n",
    "Each section follows with a brief sentence to describe what to write in this section.\n",
    "\n",
    "You need to generate a final outline based on these provided outlines.\n",
    "Return in the format:\n",
    "<format>\n",
    "Title: [TITLE OF THE SURVEY]\n",
    "Section 1: [NAME OF SECTION 1]\n",
    "Description 1: [DESCRIPTION OF SECTION 1]\n",
    "\n",
    "Section 2: [NAME OF SECTION 2]\n",
    "Description 2: [DESCRIPTION OF SECTION 2]\n",
    "\n",
    "...\n",
    "\n",
    "Section K: [NAME OF SECTION K]\n",
    "Description K: [DESCRIPTION OF SECTION K]\n",
    "</format>\n",
    "Only return the final outline without any other information:\n",
    "'''\n",
    "\n",
    "SUBSECTION_OUTLINE_PROMPT = '''\n",
    "You are an expert in artificial intelligence who wants to write an overall survey about [TOPIC].\n",
    "You have created an overall outline below:\n",
    "---\n",
    "[OVERALL OUTLINE]\n",
    "---\n",
    "The outline contains a title and several sections.\n",
    "Each section follows with a brief sentence to describe what to write in this section.\n",
    "\n",
    "<instruction>\n",
    "You need to enrich the section [SECTION NAME].\n",
    "The description of [SECTION NAME]: [SECTION DESCRIPTION]\n",
    "You need to generate the framework containing several subsections based on the overall outline.\n",
    "Each subsection follows with a brief sentence to describe what to write in this subsection.\n",
    "These papers are provided for reference:\n",
    "---\n",
    "[PAPER LIST]\n",
    "---\n",
    "Return the outline in the format:\n",
    "<format>\n",
    "Subsection 1: [NAME OF SUBSECTION 1]\n",
    "Description 1: [DESCRIPTION OF SUBSECTION 1]\n",
    "\n",
    "Subsection 2: [NAME OF SUBSECTION 2]\n",
    "Description 2: [DESCRIPTION OF SUBSECTION 2]\n",
    "\n",
    "...\n",
    "\n",
    "Subsection K: [NAME OF SUBSECTION K]\n",
    "Description K: [DESCRIPTION OF SUBSECTION K]\n",
    "</format>\n",
    "</instruction>\n",
    "Only return the outline without any other information:\n",
    "'''\n",
    "\n",
    "EDIT_FINAL_OUTLINE_PROMPT = '''\n",
    "You are an expert in artificial intelligence who wants to write an overall survey about [TOPIC].\n",
    "You have created a draft outline below:\n",
    "---\n",
    "[OVERALL OUTLINE]\n",
    "---\n",
    "The outline contains a title and several sections.\n",
    "Each section follows with a brief sentence to describe what to write in this section.\n",
    "\n",
    "Under each section, there are several subsections.\n",
    "Each subsection also follows with a brief sentence of description.\n",
    "You need to modify the outline to make it both comprehensive and coherent with no repeated subsections.\n",
    "Return the final outline in the format:\n",
    "<format>\n",
    "# [TITLE OF SURVEY]\n",
    "\n",
    "## [NAME OF SECTION 1]\n",
    "\n",
    "### [NAME OF SUBSECTION 1]\n",
    "\n",
    "### [NAME OF SUBSECTION 2]\n",
    "\n",
    "...\n",
    "\n",
    "### [NAME OF SUBSECTION L]\n",
    "\n",
    "## [NAME OF SECTION 2]\n",
    "\n",
    "...\n",
    "\n",
    "## [NAME OF SECTION K]\n",
    "\n",
    "...\n",
    "</format>\n",
    "Only return the final outline without any other information:\n",
    "'''\n",
    "\n",
    "# Subsection Writing Prompts\n",
    "SUBSECTION_WRITING_PROMPT = '''\n",
    "You are an expert in the field of \"[TOPIC]\". You are writing a survey and need to generate the content for a subsection.\n",
    "\n",
    "Subsection Title: [SUBSECTION NAME]\n",
    "Subsection Description: [DESCRIPTION]\n",
    "\n",
    "Please write a detailed and informative subsection of at least [WORD NUM] words. Ensure that the content is coherent, well-structured, and provides valuable insights into the topic.\n",
    "\n",
    "When mentioning specific papers, cite them in the format [Paper Title]. Use the provided papers as references:\n",
    "---\n",
    "[PAPER LIST]\n",
    "---\n",
    "\n",
    "Return the content without any additional explanations.\n",
    "'''\n",
    "\n",
    "LCE_PROMPT = '''\n",
    "You are an expert in artificial intelligence who wants to write an overall and comprehensive survey about [TOPIC].\n",
    "You have created an overall outline below:\n",
    "---\n",
    "[OVERALL OUTLINE]\n",
    "---\n",
    "<instruction>\n",
    "\n",
    "Now you need to help to refine one of the subsections to improve the coherence of your survey.\n",
    "\n",
    "You are provided with the content of the subsection \"[SUBSECTION NAME]\" along with the previous subsections and following subsections.\n",
    "\n",
    "Previous Subsection:\n",
    "--- \n",
    "[PREVIOUS]\n",
    "---\n",
    "\n",
    "Subsection to Refine: \n",
    "---\n",
    "[SUBSECTION]\n",
    "---\n",
    "\n",
    "Following Subsection:\n",
    "---\n",
    "[FOLLOWING]\n",
    "---\n",
    "\n",
    "If the content of Previous Subsection is empty, it means that the subsection to refine is the first subsection.\n",
    "If the content of Following Subsection is empty, it means that the subsection to refine is the last subsection.\n",
    "\n",
    "Now edit the middle subsection to enhance coherence, remove redundancies, and ensure that it connects more fluidly with the previous and following subsections. \n",
    "Please keep the essence and core information of the subsection intact. \n",
    "</instruction>\n",
    "\n",
    "Directly return the refined subsection without any other information:\n",
    "'''\n",
    "\n",
    "CHECK_CITATION_PROMPT = '''\n",
    "You are an expert in the field of \"[TOPIC]\". You have written the following subsection:\n",
    "---\n",
    "[SUBSECTION]\n",
    "---\n",
    "Please ensure that all statements that require citations are properly cited using the papers provided below:\n",
    "---\n",
    "[PAPER LIST]\n",
    "---\n",
    "If any statements are missing citations, add appropriate citations from the provided papers in the format [Paper Title].\n",
    "\n",
    "Return the updated subsection without any additional explanations.\n",
    "'''\n",
    "\n",
    "# Include any other prompts as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenCounter:\n",
    "    def __init__(self, encoding_name='gpt2'):\n",
    "        from transformers import GPT2Tokenizer\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(encoding_name)\n",
    "        logger.info(f\"TokenCounter initialized with encoding: {encoding_name}\")\n",
    "\n",
    "    def num_tokens_from_string(self, string):\n",
    "        \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "        num = len(self.tokenizer.encode(string))\n",
    "        logger.debug(f\"Counted {num} tokens in a string.\")\n",
    "        return num\n",
    "\n",
    "    def num_tokens_from_list_string(self, strings):\n",
    "        \"\"\"Returns the total number of tokens in a list of text strings.\"\"\"\n",
    "        total = sum([self.num_tokens_from_string(s) for s in strings])\n",
    "        logger.debug(f\"Counted total {total} tokens in a list of strings.\")\n",
    "        return total\n",
    "\n",
    "    def compute_price(self, input_tokens, output_tokens, model='gpt-3.5-turbo'):\n",
    "        \"\"\"\n",
    "        Computes the cost based on the number of input and output tokens.\n",
    "        Prices are based on OpenAI's pricing as of September 2023.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Computing price for model: {model}\")\n",
    "        # Define token prices per 1000 tokens (These are example prices; please check OpenAI's pricing for up-to-date values)\n",
    "        if model == 'gpt-3.5-turbo':\n",
    "            input_price_per_1k = 0.0015  # $0.0015 per 1K tokens (input)\n",
    "            output_price_per_1k = 0.002   # $0.002 per 1K tokens (output)\n",
    "        elif model == 'gpt-4':\n",
    "            input_price_per_1k = 0.03    # $0.03 per 1K tokens (input)\n",
    "            output_price_per_1k = 0.06    # $0.06 per 1K tokens (output)\n",
    "        else:\n",
    "            input_price_per_1k = 0.01    # default values\n",
    "            output_price_per_1k = 0.02\n",
    "\n",
    "        input_cost = (input_tokens / 1000) * input_price_per_1k\n",
    "        output_cost = (output_tokens / 1000) * output_price_per_1k\n",
    "        total_cost = input_cost + output_cost\n",
    "        logger.info(f\"Computed price: Input Cost = ${input_cost:.4f}, Output Cost = ${output_cost:.4f}, Total Cost = ${total_cost:.4f}\")\n",
    "        return total_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string, encoding_name='gpt2'):\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    from transformers import GPT2Tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(encoding_name)\n",
    "    return len(tokenizer.encode(string))\n",
    "\n",
    "def remove_descriptions(text):\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = [line for line in lines if not line.strip().startswith(\"Description\")]\n",
    "    result = '\\n'.join(filtered_lines)\n",
    "    return result\n",
    "\n",
    "def extract_num(string):\n",
    "    numbers = re.findall(r'\\d+', string)\n",
    "    if len(numbers) == 0:\n",
    "        return ''\n",
    "    return int(numbers[0])\n",
    "\n",
    "def generate_prompt(template, paras):\n",
    "    prompt = template\n",
    "    for k, v in paras.items():\n",
    "        prompt = prompt.replace(f'[{k}]', v)\n",
    "    return prompt\n",
    "\n",
    "def chunk_texts(texts, max_tokens):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    for idx, text in enumerate(texts):\n",
    "        text_tokens = num_tokens_from_string(text)\n",
    "        if current_tokens + text_tokens > max_tokens:\n",
    "            logger.debug(f\"Chunk {len(chunks)+1}: Adding {len(current_chunk)} texts with total tokens {current_tokens}\")\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = [text]\n",
    "            current_tokens = text_tokens\n",
    "            logger.debug(f\"Starting new chunk with text index {idx}\")\n",
    "        else:\n",
    "            current_chunk.append(text)\n",
    "            current_tokens += text_tokens\n",
    "    if current_chunk:\n",
    "        logger.debug(f\"Final Chunk {len(chunks)+1}: Adding {len(current_chunk)} texts with total tokens {current_tokens}\")\n",
    "        chunks.append(current_chunk)\n",
    "    logger.info(f\"Total chunks created: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIModel:\n",
    "    def __init__(self, model, api_key, api_url):\n",
    "        self.__api_key = api_key\n",
    "        self.__api_url = api_url\n",
    "        self.model = model\n",
    "\n",
    "    def __req(self, text, temperature, max_try=5):\n",
    "        url = f\"{self.__api_url}\"\n",
    "        pay_load_dict = {\n",
    "            \"model\": f\"{self.model}\",\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{text}\"\n",
    "            }],\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        payload = json.dumps(pay_load_dict)\n",
    "        headers = {\n",
    "            'Accept': 'application/json',\n",
    "            'Authorization': f'Bearer {self.__api_key}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        for attempt in range(max_try):\n",
    "            try:\n",
    "                response = requests.post(url, headers=headers, data=payload)\n",
    "                response.raise_for_status()\n",
    "                return response.json()['choices'][0]['message']['content']\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                time.sleep(0.2)\n",
    "        return None\n",
    "\n",
    "    def chat(self, text, temperature=1):\n",
    "        return self.__req(text, temperature=temperature)\n",
    "\n",
    "    def batch_chat(self, text_batch, temperature=0):\n",
    "        max_threads = 5  # Adjust as needed\n",
    "        res_l = ['No response'] * len(text_batch)\n",
    "        thread_l = []\n",
    "\n",
    "        def worker(i, text):\n",
    "            res_l[i] = self.chat(text, temperature)\n",
    "\n",
    "        for i, text in enumerate(text_batch):\n",
    "            thread = threading.Thread(target=worker, args=(i, text))\n",
    "            thread_l.append(thread)\n",
    "            thread.start()\n",
    "            while len(thread_l) >= max_threads:\n",
    "                thread_l = [t for t in thread_l if t.is_alive()]\n",
    "                time.sleep(0.3)\n",
    "\n",
    "        for thread in thread_l:\n",
    "            thread.join()\n",
    "        return res_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database:\n",
    "    def __init__(self, db_path, embedding_model_name):\n",
    "        logger.info(\"Initializing Database...\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name, trust_remote_code=True)\n",
    "        self.embedding_model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        logger.info(\"SentenceTransformer model loaded and moved to appropriate device.\")\n",
    "\n",
    "        self.db_path = db_path\n",
    "        self.conn = sqlite3.connect(os.path.join(self.db_path, 'arxiv_paper_db.sqlite'))\n",
    "        self.cursor = self.conn.cursor()\n",
    "        logger.info(\"Connected to SQLite database.\")\n",
    "\n",
    "        # Load FAISS indexes\n",
    "        logger.info(\"Loading FAISS indexes...\")\n",
    "        self.title_index = faiss.read_index(os.path.join(db_path, 'faiss_paper_title_embeddings.bin'))\n",
    "        with open(os.path.join(db_path, 'index_to_arxivid_title.json'), 'r') as f:\n",
    "            self.index_to_id_title = json.load(f)\n",
    "        logger.info(\"Title FAISS index and mapping loaded.\")\n",
    "\n",
    "        self.abs_index = faiss.read_index(os.path.join(db_path, 'faiss_paper_abs_embeddings.bin'))\n",
    "        with open(os.path.join(db_path, 'index_to_arxivid_abs.json'), 'r') as f:\n",
    "            self.index_to_id_abs = json.load(f)\n",
    "        logger.info(\"Abstract FAISS index and mapping loaded.\")\n",
    "\n",
    "    def close(self):\n",
    "        self.conn.close()\n",
    "        logger.info(\"Database connection closed.\")\n",
    "\n",
    "    def get_embeddings(self, texts):\n",
    "        logger.debug(f\"Generating embeddings for {len(texts)} texts.\")\n",
    "        embeddings = self.embedding_model.encode(texts, show_progress_bar=False)\n",
    "        # Ensure embeddings are in the shape (n, d)\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        return embeddings\n",
    "\n",
    "    def get_ids_from_query(self, query, num, title=True, shuffle=False):\n",
    "        logger.info(f\"Fetching top {num} IDs for query: {query}\")\n",
    "        q = self.get_embeddings([query])  # Keep as 2D array\n",
    "        ids = self.search(q, top_k=num, title=title)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(ids)\n",
    "            logger.debug(\"Shuffled the IDs.\")\n",
    "        return ids\n",
    "\n",
    "    def search(self, query_vector, top_k=1, title=True):\n",
    "        # Ensure query_vector is a 2D array\n",
    "        if query_vector.ndim == 1:\n",
    "            query_vector = query_vector.reshape(1, -1).astype('float32')\n",
    "        else:\n",
    "            query_vector = query_vector.astype('float32')\n",
    "        \n",
    "        logger.debug(f\"Query vector shape: {query_vector.shape}\")\n",
    "        \n",
    "        index = self.title_index if title else self.abs_index\n",
    "        index_to_id = self.index_to_id_title if title else self.index_to_id_abs\n",
    "        distances, indices = index.search(query_vector, top_k)\n",
    "        results = [index_to_id[str(idx)] for idx in indices[0] if idx != -1]\n",
    "        logger.debug(f\"Search returned {len(results)} results.\")\n",
    "        return results\n",
    "\n",
    "    def get_paper_info_from_ids(self, ids):\n",
    "        placeholders = ','.join('?' for _ in ids)\n",
    "        query = f\"SELECT * FROM cs_paper_info WHERE id IN ({placeholders})\"\n",
    "        self.cursor.execute(query, ids)\n",
    "        rows = self.cursor.fetchall()\n",
    "        columns = [description[0] for description in self.cursor.description]\n",
    "        result = [dict(zip(columns, row)) for row in rows]\n",
    "        logger.info(f\"Retrieved information for {len(result)} papers.\")\n",
    "        return result\n",
    "\n",
    "    def get_titles_from_citations(self, citations):\n",
    "        logger.info(f\"Fetching IDs for {len(citations)} citations.\")\n",
    "        placeholders = ','.join('?' for _ in citations)\n",
    "        query = f\"SELECT id FROM cs_paper_info WHERE title IN ({placeholders})\"\n",
    "        self.cursor.execute(query, citations)\n",
    "        ids = [row[0] for row in self.cursor.fetchall()]\n",
    "        logger.debug(f\"Found {len(ids)} matching IDs for citations.\")\n",
    "        return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class outlineWriter:\n",
    "    def __init__(self, model: str, api_key: str, api_url: str, database) -> None:\n",
    "        self.model, self.api_key, self.api_url = model, api_key, api_url \n",
    "        self.api_model = APIModel(self.model, self.api_key, self.api_url)\n",
    "        self.db = database\n",
    "        self.token_counter = tokenCounter()\n",
    "        self.input_token_usage, self.output_token_usage = 0, 0\n",
    "        logger.info(\"OutlineWriter initialized.\")\n",
    "\n",
    "    def _generate_prompt(self, template, paras):\n",
    "        prompt = template\n",
    "        for k in paras.keys():\n",
    "            prompt = prompt.replace(f'[{k}]', paras[k])\n",
    "        return prompt\n",
    "\n",
    "    def draft_outline(self, topic, reference_num=600, chunk_size=30000, section_num=6):\n",
    "        logger.info(f\"Drafting outline for topic: '{topic}' with {reference_num} references and {section_num} sections.\")\n",
    "        references_ids = self.db.get_ids_from_query(topic, num=reference_num, shuffle=True)\n",
    "        logger.debug(f\"Retrieved {len(references_ids)} reference IDs.\")\n",
    "        references_infos = self.db.get_paper_info_from_ids(references_ids)\n",
    "        logger.debug(\"Fetched paper information from database.\")\n",
    "\n",
    "        references_titles = [r['title'] for r in references_infos]\n",
    "        references_abs = [r['abs'] for r in references_infos]\n",
    "        logger.info(\"Starting chunking of references.\")\n",
    "        abs_chunks, titles_chunks = self.chunking(references_abs, references_titles, chunk_size=chunk_size)\n",
    "        logger.info(f\"Completed chunking into {len(abs_chunks)} chunks.\")\n",
    "\n",
    "        # Generate rough section-level outline\n",
    "        outlines = self.generate_rough_outlines(topic=topic, papers_chunks=abs_chunks, titles_chunks=titles_chunks, section_num=section_num)\n",
    "        logger.info(\"Generated rough section-level outlines.\")\n",
    "\n",
    "        # Merge outline\n",
    "        section_outline = self.merge_outlines(topic=topic, outlines=outlines)\n",
    "        logger.info(\"Merged outlines into a final section outline.\")\n",
    "\n",
    "        # Generate subsection-level outline\n",
    "        subsection_outlines = self.generate_subsection_outlines(topic=topic, section_outline=section_outline, rag_num=50)\n",
    "        logger.info(\"Generated subsection-level outlines.\")\n",
    "\n",
    "        # Process outlines\n",
    "        merged_outline = self.process_outlines(section_outline, subsection_outlines)\n",
    "        logger.info(\"Processed and merged section and subsection outlines.\")\n",
    "\n",
    "        # Edit final outline\n",
    "        final_outline = self.edit_final_outline(merged_outline)\n",
    "        logger.info(\"Edited final outline for coherence and comprehensiveness.\")\n",
    "\n",
    "        return final_outline\n",
    "\n",
    "    def generate_rough_outlines(self, topic, papers_chunks, titles_chunks, section_num=8):\n",
    "        logger.info(\"Generating rough outlines for each chunk.\")\n",
    "        prompts = []\n",
    "        for i in trange(len(papers_chunks), desc=\"Generating Rough Outlines\"):\n",
    "            titles = titles_chunks[i]\n",
    "            papers = papers_chunks[i]\n",
    "            paper_texts = '' \n",
    "            for idx, (t, p) in enumerate(zip(titles, papers)):\n",
    "                paper_texts += f'---\\npaper_title: {t}\\n\\npaper_content:\\n\\n{p}\\n'\n",
    "            paper_texts += '---\\n'\n",
    "            prompt = self._generate_prompt(ROUGH_OUTLINE_PROMPT, paras={'PAPER LIST': paper_texts, 'TOPIC': topic, 'SECTION NUM': str(section_num)})\n",
    "            prompts.append(prompt)\n",
    "            logger.debug(f\"Generated prompt for chunk {i+1}.\")\n",
    "        self.input_token_usage += self.token_counter.num_tokens_from_list_string(prompts)\n",
    "        logger.debug(f\"Total tokens for rough outline prompts: {self.token_counter.num_tokens_from_list_string(prompts)}\")\n",
    "        outlines = self.api_model.batch_chat(text_batch=prompts, temperature=1)\n",
    "        self.output_token_usage += self.token_counter.num_tokens_from_list_string(outlines)\n",
    "        logger.info(\"Rough outlines generated via API.\")\n",
    "        return outlines\n",
    "\n",
    "    def merge_outlines(self, topic, outlines):\n",
    "        logger.info(\"Merging multiple outlines into a final outline.\")\n",
    "        outline_texts = '' \n",
    "        for i, o in zip(range(len(outlines)), outlines):\n",
    "            outline_texts += f'---\\noutline_id: {i}\\n\\noutline_content:\\n\\n{o}\\n'\n",
    "        outline_texts += '---\\n'\n",
    "        prompt = self._generate_prompt(MERGING_OUTLINE_PROMPT, paras={'OUTLINE LIST': outline_texts, 'TOPIC': topic})\n",
    "        self.input_token_usage += self.token_counter.num_tokens_from_string(prompt)\n",
    "        logger.debug(f\"Token usage after merging prompt: {self.token_counter.num_tokens_from_string(prompt)}\")\n",
    "        outline = self.api_model.chat(prompt, temperature=1)\n",
    "        self.output_token_usage += self.token_counter.num_tokens_from_string(outline)\n",
    "        logger.info(\"Final outline merged via API.\")\n",
    "        return outline\n",
    "\n",
    "    def generate_subsection_outlines(self, topic, section_outline, rag_num):\n",
    "        logger.info(\"Generating subsection outlines for each section.\")\n",
    "        survey_title, survey_sections, survey_section_descriptions = self.extract_title_sections_descriptions(section_outline)\n",
    "        logger.debug(f\"Survey Title: {survey_title}\")\n",
    "        logger.debug(f\"Number of Sections: {len(survey_sections)}\")\n",
    "        prompts = []\n",
    "        for section_name, section_description in zip(survey_sections, survey_section_descriptions):\n",
    "            logger.debug(f\"Generating subsections for section: {section_name}\")\n",
    "            references_ids = self.db.get_ids_from_query(section_description, num=rag_num, shuffle=True)\n",
    "            references_infos = self.db.get_paper_info_from_ids(references_ids)\n",
    "            references_titles = [r['title'] for r in references_infos]\n",
    "            references_papers = [r['abs'] for r in references_infos]\n",
    "            paper_texts = '' \n",
    "            for t, p in zip(references_titles, references_papers):\n",
    "                paper_texts += f'---\\npaper_title: {t}\\n\\npaper_content:\\n\\n{p}\\n'\n",
    "            paper_texts += '---\\n'\n",
    "            prompt = self._generate_prompt(SUBSECTION_OUTLINE_PROMPT, paras={\n",
    "                'OVERALL OUTLINE': section_outline,\n",
    "                'SECTION NAME': section_name,\n",
    "                'SECTION DESCRIPTION': section_description,\n",
    "                'TOPIC': topic,\n",
    "                'PAPER LIST': paper_texts\n",
    "            })\n",
    "            prompts.append(prompt)\n",
    "            logger.debug(f\"Generated prompt for subsection in section: {section_name}\")\n",
    "        self.input_token_usage += self.token_counter.num_tokens_from_list_string(prompts)\n",
    "        logger.debug(f\"Total tokens for subsection outline prompts: {self.token_counter.num_tokens_from_list_string(prompts)}\")\n",
    "        sub_outlines = self.api_model.batch_chat(prompts, temperature=1)\n",
    "        self.output_token_usage += self.token_counter.num_tokens_from_list_string(sub_outlines)\n",
    "        logger.info(\"Subsection outlines generated via API.\")\n",
    "        return sub_outlines\n",
    "\n",
    "    def edit_final_outline(self, outline):\n",
    "        logger.info(\"Editing final outline for coherence and comprehensiveness.\")\n",
    "        prompt = self._generate_prompt(EDIT_FINAL_OUTLINE_PROMPT, paras={'OVERALL OUTLINE': outline})\n",
    "        self.input_token_usage += self.token_counter.num_tokens_from_string(prompt)\n",
    "        refined_outline = self.api_model.chat(prompt, temperature=1).replace('<format>\\n','').replace('</format>','')\n",
    "        self.output_token_usage += self.token_counter.num_tokens_from_string(refined_outline)\n",
    "        logger.info(\"Final outline edited via API.\")\n",
    "        return refined_outline\n",
    "\n",
    "    def extract_title_sections_descriptions(self, outline):\n",
    "        logger.debug(\"Extracting title, sections, and descriptions from outline.\")\n",
    "        try:\n",
    "            title = outline.split('Title: ')[1].split('\\n')[0]\n",
    "        except IndexError:\n",
    "            logger.error(\"Failed to extract title from outline.\")\n",
    "            title = \"Untitled Survey\"\n",
    "        sections, descriptions = [], []\n",
    "        for i in range(1, 101):\n",
    "            section_key = f'Section {i}: '\n",
    "            desc_key = f'Description {i}: '\n",
    "            if section_key in outline:\n",
    "                section = outline.split(section_key)[1].split('\\n')[0].strip()\n",
    "                sections.append(section)\n",
    "                if desc_key in outline:\n",
    "                    description = outline.split(desc_key)[1].split('\\n')[0].strip()\n",
    "                    descriptions.append(description)\n",
    "                    logger.debug(f\"Section {i}: {section} - Description: {description}\")\n",
    "                else:\n",
    "                    descriptions.append('')\n",
    "                    logger.debug(f\"Section {i}: {section} - No Description Found\")\n",
    "        logger.debug(f\"Extracted {len(sections)} sections.\")\n",
    "        return title, sections, descriptions\n",
    "\n",
    "    def extract_subsections_subdescriptions(self, outline):\n",
    "        logger.debug(\"Extracting subsections and their descriptions from outline.\")\n",
    "        subsections, subdescriptions = [], []\n",
    "        for i in range(1, 101):\n",
    "            subsection_key = f'Subsection {i}: '\n",
    "            subdesc_key = f'Description {i}: '\n",
    "            if subsection_key in outline:\n",
    "                subsection = outline.split(subsection_key)[1].split('\\n')[0].strip()\n",
    "                subsections.append(subsection)\n",
    "                if subdesc_key in outline:\n",
    "                    subdescription = outline.split(subdesc_key)[1].split('\\n')[0].strip()\n",
    "                    subdescriptions.append(subdescription)\n",
    "                    logger.debug(f\"Subsection {i}: {subsection} - Description: {subdescription}\")\n",
    "                else:\n",
    "                    subdescriptions.append('')\n",
    "                    logger.debug(f\"Subsection {i}: {subsection} - No Description Found\")\n",
    "        logger.debug(f\"Extracted {len(subsections)} subsections.\")\n",
    "        return subsections, subdescriptions\n",
    "\n",
    "    def chunking(self, papers, titles, chunk_size=14000):\n",
    "        logger.info(\"Starting chunking of papers and titles.\")\n",
    "        logger.debug(f\"Total papers: {len(papers)}\")\n",
    "        paper_chunks, title_chunks = [], []\n",
    "        total_length = self.token_counter.num_tokens_from_list_string(papers)\n",
    "        num_of_chunks = int(total_length / chunk_size) + 1\n",
    "        avg_len = int(total_length / num_of_chunks) + 1\n",
    "        logger.debug(f\"Total tokens: {total_length}, Number of chunks: {num_of_chunks}, Average tokens per chunk: {avg_len}\")\n",
    "        split_points = []\n",
    "        l = 0\n",
    "        for j in range(len(papers)):\n",
    "            l += self.token_counter.num_tokens_from_string(papers[j])\n",
    "            if l > avg_len:\n",
    "                l = 0\n",
    "                split_points.append(j)\n",
    "                logger.debug(f\"Split at paper index {j}\")\n",
    "                continue\n",
    "        start = 0\n",
    "        for point in split_points:\n",
    "            paper_chunks.append(papers[start:point])\n",
    "            title_chunks.append(titles[start:point])\n",
    "            logger.debug(f\"Chunk {len(paper_chunks)}: Papers {start} to {point-1}\")\n",
    "            start = point\n",
    "        paper_chunks.append(papers[start:])\n",
    "        title_chunks.append(titles[start:])\n",
    "        logger.debug(f\"Final chunk {len(paper_chunks)}: Papers {start} to end\")\n",
    "        logger.info(\"Completed chunking of papers and titles.\")\n",
    "        return paper_chunks, title_chunks\n",
    "\n",
    "    def process_outlines(self, section_outline, sub_outlines):\n",
    "        logger.info(\"Processing and merging section and subsection outlines into final survey outline.\")\n",
    "        survey_title, survey_sections, survey_section_descriptions = self.extract_title_sections_descriptions(outline=section_outline)\n",
    "        logger.debug(f\"Survey Title: {survey_title}\")\n",
    "        logger.debug(f\"Number of Sections: {len(survey_sections)}\")\n",
    "        res = f'# {survey_title}\\n\\n'\n",
    "        for i in range(len(survey_sections)):\n",
    "            section = survey_sections[i]\n",
    "            res += f'## {i+1} {section}\\nDescription: {survey_section_descriptions[i]}\\n\\n'\n",
    "            subsections, subsection_descriptions = self.extract_subsections_subdescriptions(sub_outlines[i])\n",
    "            for j in range(len(subsections)):\n",
    "                subsection = subsections[j]\n",
    "                res += f'### {i+1}.{j+1} {subsection}\\nDescription: {subsection_descriptions[j]}\\n\\n'\n",
    "                logger.debug(f\"Added subsection {j+1} to section {i+1}: {subsection}\")\n",
    "        logger.info(\"Final survey outline constructed.\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class subsectionWriter:\n",
    "    def __init__(self, model: str, api_key: str, api_url: str, database) -> None:\n",
    "        self.model, self.api_key, self.api_url = model, api_key, api_url\n",
    "        self.api_model = APIModel(self.model, self.api_key, self.api_url)\n",
    "        self.db = database\n",
    "        self.token_counter = tokenCounter()\n",
    "        self.input_token_usage, self.output_token_usage = 0, 0\n",
    "        logger.info(\"subsectionWriter initialized.\")\n",
    "\n",
    "    def _generate_prompt(self, template, paras):\n",
    "        prompt = template\n",
    "        for k in paras.keys():\n",
    "            prompt = prompt.replace(f'[{k}]', paras[k])\n",
    "        return prompt\n",
    "\n",
    "    def write(self, topic, outline, rag_num=30, subsection_len=500, refining=True, reflection=True):\n",
    "        logger.info(f\"Starting to write subsections for topic: '{topic}' with rag_num={rag_num} and subsection_len={subsection_len}.\")\n",
    "        # Parse the outline to get sections and subsections\n",
    "        parsed_outline = self.parse_outline(outline=outline)\n",
    "        logger.debug(\"Parsed Outline:\")\n",
    "        logger.debug(json.dumps(parsed_outline, indent=2))\n",
    "\n",
    "        # Initialize structures\n",
    "        section_content = [[] for _ in range(len(parsed_outline['sections']))]\n",
    "        section_paper_texts = [[] for _ in range(len(parsed_outline['sections']))]\n",
    "\n",
    "        total_ids = []\n",
    "        section_references_ids = [[] for _ in range(len(parsed_outline['sections']))]\n",
    "        for i, sub_desc in enumerate(parsed_outline['subsection_descriptions']):\n",
    "            for d in sub_desc:\n",
    "                references_ids = self.db.get_ids_from_query(d, num=rag_num, shuffle=False)\n",
    "                total_ids += references_ids\n",
    "                section_references_ids[i].append(references_ids)\n",
    "                logger.debug(f\"Section {i+1}: Retrieved {len(references_ids)} references for description.\")\n",
    "\n",
    "        # Fetch unique references\n",
    "        unique_ids = list(set(total_ids))\n",
    "        logger.info(f\"Total unique references fetched: {len(unique_ids)}\")\n",
    "        total_references_infos = self.db.get_paper_info_from_ids(unique_ids)\n",
    "        logger.debug(\"Fetched all unique paper information from database.\")\n",
    "        temp_title_dic = {p['id']: p['title'] for p in total_references_infos}\n",
    "        temp_abs_dic = {p['id']: p['abs'] for p in total_references_infos}\n",
    "\n",
    "        # Prepare paper texts for each section\n",
    "        for i in range(len(parsed_outline['sections'])):\n",
    "            for references_ids in section_references_ids[i]:\n",
    "                references_titles = [temp_title_dic[_] for _ in references_ids]\n",
    "                references_papers = [temp_abs_dic[_] for _ in references_ids]\n",
    "                paper_texts = '' \n",
    "                for t, p in zip(references_titles, references_papers):\n",
    "                    paper_texts += f'---\\n\\npaper_title: {t}\\n\\npaper_content:\\n\\n{p}\\n'\n",
    "                paper_texts += '---\\n'\n",
    "                section_paper_texts[i].append(paper_texts)\n",
    "                logger.debug(f\"Section {i+1}: Prepared paper texts for {len(references_ids)} references.\")\n",
    "\n",
    "        # Start threads to write subsections\n",
    "        thread_l = []\n",
    "        for i in range(len(parsed_outline['sections'])):\n",
    "            thread = threading.Thread(target=self.write_subsection_with_reflection, args=(\n",
    "                section_paper_texts[i], topic, outline, \n",
    "                parsed_outline['sections'][i], \n",
    "                parsed_outline['subsections'][i], \n",
    "                parsed_outline['subsection_descriptions'][i], \n",
    "                section_content, i, rag_num, str(subsection_len)\n",
    "            ))\n",
    "            thread_l.append(thread)\n",
    "            thread.start()\n",
    "            logger.debug(f\"Started thread for Section {i+1}.\")\n",
    "            time.sleep(0.1)  # Slight delay to prevent overwhelming the API\n",
    "\n",
    "        # Wait for all threads to complete\n",
    "        for thread in thread_l:\n",
    "            thread.join()\n",
    "        logger.info(\"All subsection threads have completed.\")\n",
    "\n",
    "        # Generate the raw survey document\n",
    "        raw_survey = self.generate_document(parsed_outline, section_content)\n",
    "        logger.debug(\"Generated raw survey document.\")\n",
    "        \n",
    "        # Process references\n",
    "        raw_survey_with_references, raw_references = self.process_references(raw_survey)\n",
    "        logger.info(\"Processed references for raw survey.\")\n",
    "\n",
    "        if refining:\n",
    "            logger.info(\"Starting refinement of subsections.\")\n",
    "            final_section_content = self.refine_subsections(topic, outline, section_content)\n",
    "            refined_survey = self.generate_document(parsed_outline, final_section_content)\n",
    "            logger.debug(\"Generated refined survey document.\")\n",
    "            refined_survey_with_references, refined_references = self.process_references(refined_survey)\n",
    "            logger.info(\"Processed references for refined survey.\")\n",
    "            return raw_survey + '\\n', raw_survey_with_references + '\\n', raw_references, refined_survey + '\\n', refined_survey_with_references + '\\n', refined_references\n",
    "        else:\n",
    "            return raw_survey + '\\n', raw_survey_with_references + '\\n', raw_references\n",
    "\n",
    "    def refine_subsections(self, topic, outline, section_content):\n",
    "        logger.info(\"Refining subsections for coherence and consistency.\")\n",
    "        section_content_even = copy.deepcopy(section_content)\n",
    "        logger.debug(\"Created a deep copy of section_content for even indexing.\")\n",
    "\n",
    "        thread_l = []\n",
    "        for i in range(len(section_content)):\n",
    "            for j in range(len(section_content[i])):\n",
    "                if j % 2 == 0:\n",
    "                    if j == 0:\n",
    "                        contents = ['', section_content[i][j], section_content[i][j+1] if j+1 < len(section_content[i]) else '']\n",
    "                    elif j == (len(section_content[i]) - 1):\n",
    "                        contents = [section_content[i][j-1], section_content[i][j], '']\n",
    "                    else:\n",
    "                        contents = [section_content[i][j-1], section_content[i][j], section_content[i][j+1]]\n",
    "                    thread = threading.Thread(target=self.lce, args=(topic, outline, contents, section_content_even[i], j))\n",
    "                    thread_l.append(thread)\n",
    "                    thread.start()\n",
    "                    logger.debug(f\"Started LCE thread for Section {i+1}, Subsection {j+1} (even).\")\n",
    "\n",
    "        for thread in thread_l:\n",
    "            thread.join()\n",
    "        logger.info(\"Completed first pass of subsection refinement (even indices).\")\n",
    "\n",
    "        final_section_content = copy.deepcopy(section_content_even)\n",
    "        thread_l = []\n",
    "        for i in range(len(section_content_even)):\n",
    "            for j in range(len(section_content_even[i])):\n",
    "                if j % 2 == 1:\n",
    "                    if j == (len(section_content_even[i]) - 1):\n",
    "                        contents = [section_content_even[i][j-1], section_content_even[i][j], '']\n",
    "                    else:\n",
    "                        contents = [section_content_even[i][j-1], section_content_even[i][j], section_content_even[i][j+1]]\n",
    "                    thread = threading.Thread(target=self.lce, args=(topic, outline, contents, final_section_content[i], j))\n",
    "                    thread_l.append(thread)\n",
    "                    thread.start()\n",
    "                    logger.debug(f\"Started LCE thread for Section {i+1}, Subsection {j+1} (odd).\")\n",
    "\n",
    "        for thread in thread_l:\n",
    "            thread.join()\n",
    "        logger.info(\"Completed second pass of subsection refinement (odd indices).\")\n",
    "        \n",
    "        return final_section_content\n",
    "\n",
    "    def write_subsection_with_reflection(self, paper_texts_l, topic, outline, section, subsections, subdescriptions, res_l, idx, rag_num=20, subsection_len=1000, citation_num=8):\n",
    "        logger.info(f\"Writing subsections for Section '{section}'.\")\n",
    "        prompts = []\n",
    "        for j in range(len(subsections)):\n",
    "            subsection = subsections[j]\n",
    "            description = subdescriptions[j]\n",
    "\n",
    "            prompt = self._generate_prompt(SUBSECTION_WRITING_PROMPT, paras={\n",
    "                'OVERALL OUTLINE': outline,\n",
    "                'SUBSECTION NAME': subsection,\n",
    "                'DESCRIPTION': description,\n",
    "                'TOPIC': topic,\n",
    "                'PAPER LIST': paper_texts_l[j],\n",
    "                'SECTION NAME': section,\n",
    "                'WORD NUM': str(subsection_len),\n",
    "                'CITATION NUM': str(citation_num)\n",
    "            })\n",
    "            prompts.append(prompt)\n",
    "            logger.debug(f\"Generated prompt for Subsection {j+1}: '{subsection}'.\")\n",
    "\n",
    "        self.input_token_usage += self.token_counter.num_tokens_from_list_string(prompts)\n",
    "        logger.debug(f\"Total tokens for subsection prompts: {self.token_counter.num_tokens_from_list_string(prompts)}\")\n",
    "        contents = self.api_model.batch_chat(prompts, temperature=1)\n",
    "        self.output_token_usage += self.token_counter.num_tokens_from_list_string(contents)\n",
    "        logger.info(f\"Generated content for Section '{section}' subsections via API.\")\n",
    "\n",
    "        # Clean up content\n",
    "        contents = [c.replace('<format>', '').replace('</format>', '') for c in contents]\n",
    "\n",
    "        # Check and add citations\n",
    "        prompts = []\n",
    "        for content, paper_texts in zip(contents, paper_texts_l):\n",
    "            prompts.append(self._generate_prompt(CHECK_CITATION_PROMPT, paras={'SUBSECTION': content, 'TOPIC': topic, 'PAPER LIST': paper_texts}))\n",
    "            logger.debug(\"Generated citation checking prompt.\")\n",
    "\n",
    "        self.input_token_usage += self.token_counter.num_tokens_from_list_string(prompts)\n",
    "        contents = self.api_model.batch_chat(prompts, temperature=1)\n",
    "        self.output_token_usage += self.token_counter.num_tokens_from_list_string(contents)\n",
    "        logger.info(f\"Checked and updated citations for Section '{section}' subsections via API.\")\n",
    "\n",
    "        # Clean up citation-checked content\n",
    "        contents = [c.replace('<format>', '').replace('</format>', '') for c in contents]\n",
    "\n",
    "        # Assign the processed content to the result list\n",
    "        res_l[idx] = contents\n",
    "        logger.debug(f\"Assigned processed content to section {idx+1}.\")\n",
    "        return contents\n",
    "\n",
    "    def lce(self, topic, outline, contents, res_l, idx):\n",
    "        logger.info(f\"Refining subsection at index {idx}.\")\n",
    "        prompt = self._generate_prompt(LCE_PROMPT, paras={\n",
    "            'OVERALL OUTLINE': outline,\n",
    "            'PREVIOUS': contents[0],\n",
    "            'FOLLOWING': contents[2],\n",
    "            'TOPIC': topic,\n",
    "            'SUBSECTION': contents[1]\n",
    "        })\n",
    "        self.input_token_usage += self.token_counter.num_tokens_from_string(prompt)\n",
    "        refined_content = self.api_model.chat(prompt, temperature=1).replace('<format>', '').replace('</format>', '')\n",
    "        self.output_token_usage += self.token_counter.num_tokens_from_string(refined_content)\n",
    "        logger.debug(f\"Refined subsection {idx+1}: {refined_content[:100]}...\")  # Log first 100 characters\n",
    "        res_l[idx] = refined_content\n",
    "        return refined_content\n",
    "\n",
    "    def parse_outline(self, outline):\n",
    "        logger.debug(\"Parsing the outline.\")\n",
    "        result = {\n",
    "            \"title\": \"\",\n",
    "            \"sections\": [],\n",
    "            \"section_descriptions\": [],\n",
    "            \"subsections\": [],\n",
    "            \"subsection_descriptions\": []\n",
    "        }\n",
    "\n",
    "        lines = outline.strip().split('\\n')\n",
    "        current_section_index = -1\n",
    "        current_subsection_index = -1\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if line.startswith('# '):\n",
    "                result[\"title\"] = line[2:].strip()\n",
    "                logger.debug(f\"Found title: {result['title']}\")\n",
    "            elif line.startswith('## '):\n",
    "                section_title = line[3:].strip()\n",
    "                result[\"sections\"].append(section_title)\n",
    "                result[\"section_descriptions\"].append(\"\")  # Placeholder for description\n",
    "                result[\"subsections\"].append([])\n",
    "                result[\"subsection_descriptions\"].append([])\n",
    "                current_section_index += 1\n",
    "                current_subsection_index = -1\n",
    "                logger.debug(f\"Found section {current_section_index +1}: {section_title}\")\n",
    "                # Check if next line is 'Description:'\n",
    "                if i + 1 < len(lines) and lines[i + 1].startswith('Description:'):\n",
    "                    description = lines[i + 1].split('Description:', 1)[1].strip()\n",
    "                    result[\"section_descriptions\"][current_section_index] = description\n",
    "                    logger.debug(f\"Found description for section {current_section_index +1}: {description}\")\n",
    "            elif line.startswith('### '):\n",
    "                subsection_title = line[4:].strip()\n",
    "                result[\"subsections\"][current_section_index].append(subsection_title)\n",
    "                result[\"subsection_descriptions\"][current_section_index].append(\"\")\n",
    "                current_subsection_index += 1\n",
    "                logger.debug(f\"Found subsection {current_subsection_index +1} in section {current_section_index +1}: {subsection_title}\")\n",
    "                # Check if next line is 'Description:'\n",
    "                if i + 1 < len(lines) and lines[i + 1].startswith('Description:'):\n",
    "                    description = lines[i + 1].split('Description:', 1)[1].strip()\n",
    "                    result[\"subsection_descriptions\"][current_section_index][current_subsection_index] = description\n",
    "                    logger.debug(f\"Found description for subsection {current_subsection_index +1} in section {current_section_index +1}: {description}\")\n",
    "\n",
    "        logger.info(\"Completed parsing the outline.\")\n",
    "        return result\n",
    "\n",
    "    def process_references(self, survey):\n",
    "        logger.info(\"Processing references in the survey.\")\n",
    "        citations = self.extract_citations(survey)\n",
    "        logger.debug(f\"Extracted {len(citations)} citations.\")\n",
    "        updated_survey, references = self.replace_citations_with_numbers(citations, survey)\n",
    "        logger.debug(\"Replaced citations with numbered references.\")\n",
    "        return updated_survey, references\n",
    "\n",
    "    def extract_citations(self, markdown_text):\n",
    "        logger.debug(\"Extracting citations from markdown text.\")\n",
    "        # Regular expression to match citations within square brackets\n",
    "        pattern = re.compile(r'\\[(.*?)\\]')\n",
    "        matches = pattern.findall(markdown_text)\n",
    "        citations = []\n",
    "        for match in matches:\n",
    "            # Exclude matches that are likely not citations (e.g., text with asterisks)\n",
    "            if '*' in match:  # Simple heuristic: skip if asterisks are present\n",
    "                logger.debug(f\"Skipping non-citation match: {match}\")\n",
    "                continue\n",
    "            parts = match.split(';')\n",
    "            for part in parts:\n",
    "                cit = part.strip()\n",
    "                if cit and cit not in citations:\n",
    "                    citations.append(cit)\n",
    "        logger.debug(f\"Filtered citations: {citations}\")\n",
    "        return citations\n",
    "\n",
    "    def replace_citations_with_numbers(self, citations, markdown_text):\n",
    "        logger.info(\"Replacing citations with numbered references.\")\n",
    "        ids = self.db.get_titles_from_citations(citations)\n",
    "        citation_to_ids = {citation: idx for citation, idx in zip(citations, ids)}\n",
    "        logger.debug(f\"Mapped citations to IDs: {citation_to_ids}\")\n",
    "\n",
    "        paper_infos = self.db.get_paper_info_from_ids(ids)\n",
    "        temp_dic = {p['id']: p['title'] for p in paper_infos}\n",
    "\n",
    "        titles = [temp_dic.get(_, '') for _ in ids]\n",
    "        logger.debug(f\"Retrieved titles for IDs: {titles}\")\n",
    "\n",
    "        ids_to_titles = {idx: title for idx, title in zip(ids, titles)}\n",
    "        titles_to_ids = {title: idx for idx, title in ids_to_titles.items()}\n",
    "        title_to_number = {title: num+1 for num, title in enumerate(titles)}\n",
    "\n",
    "        number_to_title = {num: title for title, num in title_to_number.items()}\n",
    "        number_to_title_sorted = {key: number_to_title[key] for key in sorted(number_to_title)}\n",
    "\n",
    "        def replace_match(match):\n",
    "            citation_text = match.group(1)\n",
    "            individual_citations = citation_text.split(';')\n",
    "            numbered_citations = []\n",
    "            for citation in individual_citations:\n",
    "                citation = citation.strip()\n",
    "                try:\n",
    "                    paper_id = citation_to_ids[citation]\n",
    "                    title = ids_to_titles[paper_id]\n",
    "                    number = title_to_number[title]\n",
    "                    numbered_citations.append(str(number))\n",
    "                except KeyError:\n",
    "                    logger.warning(f\"Citation not found in database: '{citation}'\")\n",
    "                    # Optionally, keep the original citation or skip\n",
    "                    # Here, we'll skip it\n",
    "                    continue\n",
    "            if numbered_citations:\n",
    "                return '[' + '; '.join(numbered_citations) + ']'\n",
    "            else:\n",
    "                return match.group(0)\n",
    "\n",
    "        updated_text = re.sub(r'\\[(.*?)\\]', replace_match, markdown_text)\n",
    "        logger.debug(\"Citations replaced with numbers in survey text.\")\n",
    "\n",
    "        # Generate the references section\n",
    "        references_section = \"\\n\\n## References\\n\\n\"\n",
    "        references = {num: titles_to_ids[title] for num, title in number_to_title_sorted.items()}\n",
    "        for idx, title in number_to_title_sorted.items():\n",
    "            t = title.replace('\\n', '')\n",
    "            references_section += f\"[{idx}] {t}\\n\\n\"\n",
    "        logger.info(\"References section created.\")\n",
    "\n",
    "        return updated_text + references_section, references\n",
    "\n",
    "    def generate_document(self, parsed_outline, subsection_contents):\n",
    "        logger.info(\"Generating the final survey document.\")\n",
    "        document = []\n",
    "        \n",
    "        # Append title\n",
    "        title = parsed_outline['title']\n",
    "        document.append(f\"# {title}\\n\")\n",
    "        logger.debug(f\"Added title: {title}\")\n",
    "        \n",
    "        # Iterate over sections and their content\n",
    "        for i, section in enumerate(parsed_outline['sections']):\n",
    "            document.append(f\"## {section}\\n\")\n",
    "            logger.debug(f\"Added section {i+1}: {section}\")\n",
    "            \n",
    "            # Append subsections and their contents\n",
    "            for j, subsection in enumerate(parsed_outline['subsections'][i]):\n",
    "                document.append(f\"### {subsection}\\n\")\n",
    "                logger.debug(f\"Added subsection {i+1}.{j+1}: {subsection}\")\n",
    "                \n",
    "                # Append detailed content for each subsection\n",
    "                if i < len(subsection_contents) and j < len(subsection_contents[i]):\n",
    "                    content = subsection_contents[i][j]\n",
    "                    document.append(content + \"\\n\")\n",
    "                    logger.debug(f\"Added content for subsection {i+1}.{j+1}\")\n",
    "        \n",
    "        final_document = \"\\n\".join(document)\n",
    "        logger.info(\"Final survey document generated.\")\n",
    "        return final_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judge:\n",
    "    def __init__(self, model, api_key, api_url, database):\n",
    "        self.model = APIModel(model, api_key, api_url)\n",
    "        self.db = database\n",
    "\n",
    "    def criteria_based_judging(self, survey, topic, criterion):\n",
    "        criterion_paras = CRITERIA[criterion]\n",
    "        content_paras = {\n",
    "            'TOPIC': topic,\n",
    "            'SURVEY': survey,\n",
    "            'Criterion Description': criterion_paras['description']\n",
    "        }\n",
    "        for score in range(1, 6):\n",
    "            content_paras[f'Score {score} Description'] = criterion_paras[f'score {score}']\n",
    "        prompt = generate_prompt(CRITERIA_BASED_JUDGING_PROMPT, content_paras)\n",
    "        response = self.model.chat(prompt, temperature=0)\n",
    "        score = extract_num(response)\n",
    "        return score\n",
    "\n",
    "    def batch_criteria_based_judging(self, survey, topic, criteria):\n",
    "        scores = []\n",
    "        for criterion in criteria:\n",
    "            score = self.criteria_based_judging(survey, topic, criterion)\n",
    "            scores.append(score)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outline(topic, model, section_num, outline_reference_num, db, api_key, api_url):\n",
    "    outline_writer = outlineWriter(model=model, api_key=api_key, api_url=api_url, database=db)\n",
    "    outline = outline_writer.draft_outline(topic, reference_num=outline_reference_num, section_num=section_num)\n",
    "    outline_without_descriptions = remove_descriptions(outline)\n",
    "    return outline, outline_without_descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_subsection(topic, model, outline, subsection_len, rag_num, db, api_key, api_url, refinement=True):\n",
    "    subsection_writer = subsectionWriter(model=model, api_key=api_key, api_url=api_url, database=db)\n",
    "    outputs = subsection_writer.write(\n",
    "        topic=topic,\n",
    "        outline=outline,\n",
    "        subsection_len=subsection_len,\n",
    "        rag_num=rag_num,\n",
    "        refining=refinement\n",
    "    )\n",
    "    if refinement:\n",
    "        survey_content, survey_with_references, references, refined_survey, refined_survey_with_references, refined_references = outputs\n",
    "        return survey_content, survey_with_references, references, refined_survey, refined_survey_with_references, refined_references\n",
    "    else:\n",
    "        survey_content, survey_with_references, references = outputs\n",
    "        return survey_content, survey_with_references, references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (Replace these with your own values or collect them via input)\n",
    "db_path = './database'\n",
    "embedding_model = 'nomic-ai/nomic-embed-text-v1'\n",
    "saving_path = './output'\n",
    "model_name = 'deepseek-chat'\n",
    "topic = 'RAG Chunking in Large Language Models'  # Example topic\n",
    "section_num = 2\n",
    "subsection_len = 300\n",
    "outline_reference_num = 100\n",
    "rag_num = 60\n",
    "api_url = 'https://api.deepseek.com/chat/completions'\n",
    "api_key = 'sk-129f52682afe4b73b557c9083a618e16'  # Your API key here\n",
    "\n",
    "# Validate API Key\n",
    "if not api_key:\n",
    "    logger.error(\"API key is required.\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    logger.info(\"API key provided.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Database\n",
    "db = Database(db_path=db_path, embedding_model_name=embedding_model)\n",
    "logger.info(\"Database initialized.\")\n",
    "\n",
    "# Ensure saving path exists\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "logger.info(f\"Saving path '{saving_path}' is ready.\")\n",
    "\n",
    "# Initialize API Model\n",
    "api_model = APIModel(model=model_name, api_key=api_key, api_url=api_url)\n",
    "logger.info(\"API Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Outline\n",
    "logger.info(\"Generating outline...\")\n",
    "outline_with_description, outline_wo_description = generate_outline(\n",
    "    topic=topic,\n",
    "    model=model_name,\n",
    "    section_num=section_num,\n",
    "    outline_reference_num=outline_reference_num,\n",
    "    db=db,\n",
    "    api_key=api_key,\n",
    "    api_url=api_url\n",
    ")\n",
    "logger.info(\"Outline generated.\")\n",
    "logger.debug(f\"Outline with descriptions:\\n{outline_with_description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outline_with_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Subsections\n",
    "logger.info(\"Writing subsections...\")\n",
    "outputs = write_subsection(\n",
    "    topic=topic,\n",
    "    model=model_name,\n",
    "    outline=outline_with_description,\n",
    "    subsection_len=subsection_len,\n",
    "    rag_num=rag_num,\n",
    "    db=db,\n",
    "    api_key=api_key,\n",
    "    api_url=api_url,\n",
    "    refinement=True\n",
    ")\n",
    "if len(outputs) == 6:\n",
    "    raw_survey, raw_survey_with_references, raw_references, refined_survey, refined_survey_with_references, refined_references = outputs\n",
    "    logger.info(\"Subsections written with refinement.\")\n",
    "else:\n",
    "    raw_survey, raw_survey_with_references, raw_references = outputs\n",
    "    logger.info(\"Subsections written without refinement.\")\n",
    "logger.debug(f\"Raw Survey:\\n{raw_survey}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the survey to a file\n",
    "survey_filename = os.path.join(saving_path, f\"{topic}.md\")\n",
    "with open(survey_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(refined_survey_with_references)\n",
    "print(f\"Survey saved to {survey_filename}\")\n",
    "\n",
    "# Save references to a JSON file\n",
    "references_filename = os.path.join(saving_path, f\"{topic}.json\")\n",
    "save_dic = {\n",
    "    'survey': refined_survey_with_references,\n",
    "    'reference': refined_references\n",
    "}\n",
    "with open(references_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(save_dic, f, indent=4)\n",
    "print(f\"References saved to {references_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the generated survey\n",
    "print(\"Evaluating survey...\")\n",
    "judge = Judge(model=model_name, api_key=api_key, api_url=api_url, database=db)\n",
    "survey_content = refined_survey_with_references\n",
    "criteria = ['Coverage', 'Structure', 'Relevance']\n",
    "scores = judge.batch_criteria_based_judging(survey_content, topic, criteria)\n",
    "\n",
    "# Save Evaluation Results\n",
    "evaluation_filename = os.path.join(saving_path, f\"{topic}_evaluation.txt\")\n",
    "with open(evaluation_filename, 'w', encoding='utf-8') as f:\n",
    "    result = f\"Evaluation Results for '{topic}'\\n\"\n",
    "    for c, s in zip(criteria, scores):\n",
    "        result += f\"{c}: {s}\\n\"\n",
    "    f.write(result)\n",
    "print(f\"Evaluation results saved to {evaluation_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Database Connection\n",
    "db.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
